{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f110a7f",
   "metadata": {},
   "source": [
    "# Setup for single factor experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d9bac",
   "metadata": {},
   "source": [
    "Using only Cardiff only and changing seeds with using LIME and SHAP default values\n",
    "\n",
    "Will only be done on google colab T4 gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install lime shap hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9e277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c62e5f5a6b4da1922538117ec938f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>error</th>\n",
       "      <th>cardiff_label</th>\n",
       "      <th>cardiff_score</th>\n",
       "      <th>lime_cardiff</th>\n",
       "      <th>shap_cardiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "      <td>None</td>\n",
       "      <td>HATE</td>\n",
       "      <td>0.935527</td>\n",
       "      <td>niggers: -0.907, nawt: -0.102, me: 0.074, yall...</td>\n",
       "      <td>n: 0.010, aw: 0.006, t : 0.007, y: 0.003, all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and this is why i end up with nigger trainee d...</td>\n",
       "      <td>None</td>\n",
       "      <td>HATE</td>\n",
       "      <td>0.993975</td>\n",
       "      <td>nigger: -0.981, with: -0.007, i: 0.004, doctor...</td>\n",
       "      <td>and : 0.003, this : 0.001, n: -0.007, igger : ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is a neutral sentence</td>\n",
       "      <td>None</td>\n",
       "      <td>NOT-HATE</td>\n",
       "      <td>0.999157</td>\n",
       "      <td>a: 0.001, is: 0.001, this: 0.001, sentence: 0....</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text error cardiff_label  \\\n",
       "0                      nawt yall niggers ignoring me  None          HATE   \n",
       "1  and this is why i end up with nigger trainee d...  None          HATE   \n",
       "2                         this is a neutral sentence  None      NOT-HATE   \n",
       "\n",
       "   cardiff_score                                       lime_cardiff  \\\n",
       "0       0.935527  niggers: -0.907, nawt: -0.102, me: 0.074, yall...   \n",
       "1       0.993975  nigger: -0.981, with: -0.007, i: 0.004, doctor...   \n",
       "2       0.999157  a: 0.001, is: 0.001, this: 0.001, sentence: 0....   \n",
       "\n",
       "                                        shap_cardiff  \n",
       "0  n: 0.010, aw: 0.006, t : 0.007, y: 0.003, all ...  \n",
       "1  and : 0.003, this : 0.001, n: -0.007, igger : ...  \n",
       "2                                                     "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity test\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import random\n",
    "import torch\n",
    "from typing import List, Union\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        # Optional: for determinism with CuDNN\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Constants\n",
    "CLASS_NAMES = ['NOT-HATE', 'HATE']\n",
    "BATCH_SIZE = 512  # Adjust based on your GPU memory\n",
    "\n",
    "# Load models and tokenizers once\n",
    "pipe_cardiff = pipeline(\"text-classification\", \n",
    "                       model=\"cardiffnlp/twitter-roberta-base-hate-latest\", \n",
    "                       device=0 if torch.cuda.is_available() else -1,\n",
    "                       batch_size=BATCH_SIZE)\n",
    "\n",
    "tokenizer_cardiff = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-hate-latest\")\n",
    "\n",
    "# Create dataset class for more efficient processing\n",
    "class HateSpeechDataset:\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "\n",
    "# Unified prediction function for both models\n",
    "def batch_predict(texts: Union[List[str], np.ndarray], pipeline_fn):\n",
    "    \"\"\"Run predictions in batches for efficiency\"\"\"\n",
    "    if isinstance(texts, np.ndarray):\n",
    "        texts = texts.tolist()\n",
    "        \n",
    "    dataset = HateSpeechDataset(texts)\n",
    "    \n",
    "    # Process in batches\n",
    "    all_outputs = []\n",
    "    for i in range(0, len(dataset), BATCH_SIZE):\n",
    "        batch_texts = [dataset[j] for j in range(i, min(i + BATCH_SIZE, len(dataset)))]\n",
    "        outputs = pipeline_fn(batch_texts, top_k=2)\n",
    "        all_outputs.extend(outputs)\n",
    "    \n",
    "    # Convert to numpy array with consistent order\n",
    "    return np.array([\n",
    "        [label['score'] for label in sorted(res, key=lambda x: x['label'])]\n",
    "        for res in all_outputs\n",
    "    ])\n",
    "\n",
    "\n",
    "# Model-specific prediction functions\n",
    "def predict_cardiff(texts):\n",
    "    return batch_predict(texts, pipe_cardiff)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize explainers\n",
    "explainer_lime = LimeTextExplainer(class_names=CLASS_NAMES)\n",
    "explainer_shap_cardiff = shap.Explainer(predict_cardiff, \n",
    "                                       masker=shap.maskers.Text(tokenizer_cardiff))\n",
    "\n",
    "# Helper function for LIME explanations\n",
    "def lime_explain(text, predictor, num_features=4):\n",
    "    exp = explainer_lime.explain_instance(text, \n",
    "                                          predictor)\n",
    "    return \", \".join([f\"{word}: {weight:.3f}\" for word, weight in exp.as_list()])\n",
    "\n",
    "\n",
    "# Helper function for SHAP explanations\n",
    "def get_shap_values(text, explainer, class_idx=1):\n",
    "    shap_vals = explainer([text])\n",
    "    return \", \".join([\n",
    "        f\"{feature}: {value:.3f}\"\n",
    "        for feature, value in zip(shap_vals[0].data, shap_vals[0].values[:, class_idx])\n",
    "        if abs(value) > 0.001\n",
    "    ])\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'text': [\n",
    "        \"nawt yall niggers ignoring me\",\n",
    "        \"and this is why i end up with nigger trainee doctors who can not speak properly lack basic knowledge of biology it truly scary if the public only knew\",\n",
    "        \"this is a neutral sentence\"\n",
    "    ]\n",
    "}\n",
    "annotated_and_targeted_hatespeech_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Process all texts in one batch for efficiency\n",
    "all_texts = annotated_and_targeted_hatespeech_df[\"text\"].tolist()\n",
    "\n",
    "# Get predictions in batch\n",
    "cardiff_preds = pipe_cardiff(all_texts) \n",
    "\n",
    "# Process results\n",
    "results = []\n",
    "for i, text in enumerate(all_texts):\n",
    "    try:\n",
    "        # Get predictions from the batch results\n",
    "        pred_cardiff = cardiff_preds[i]\n",
    "\n",
    "        # LIME explanations\n",
    "        lime_exp_cardiff = lime_explain(text, predict_cardiff)\n",
    "\n",
    "        # SHAP explanations\n",
    "        shap_exp_cardiff = get_shap_values(text, explainer_shap_cardiff)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"error\": None,\n",
    "            \"cardiff_label\": pred_cardiff[\"label\"],\n",
    "            \"cardiff_score\": pred_cardiff[\"score\"],\n",
    "            \"lime_cardiff\": lime_exp_cardiff,\n",
    "            \"shap_cardiff\": shap_exp_cardiff\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"error\": f\"Caught an exception: {e}\",\n",
    "            \"cardiff_label\": None,\n",
    "            \"cardiff_score\": None,\n",
    "            \"lime_cardiff\": None,\n",
    "            \"shap_cardiff\": None\n",
    "        })\n",
    "\n",
    "final_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41244756",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Saving to drive\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Save to your Google Drive\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Saving to drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Save to your Google Drive\n",
    "final_df.to_csv('/content/drive/MyDrive/hate_speech_analysis_results.csv', index=False)\n",
    "final_df.to_excel('/content/drive/MyDrive/hate_speech_analysis_results.xlsx', index=False)\n",
    "final_df.to_pickle('/content/drive/MyDrive/hate_speech_analysis_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fbef335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded successfully with 65217 rows\n",
      "\n",
      "Preview of the DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_new</th>\n",
       "      <th>text</th>\n",
       "      <th>extracted_target</th>\n",
       "      <th>label_hatespeech_binary_offensive_not_included</th>\n",
       "      <th>label_hatespeech_binary_offensive_included</th>\n",
       "      <th>label_normal_offensive_hatespeech</th>\n",
       "      <th>platform</th>\n",
       "      <th>original_dataset_title</th>\n",
       "      <th>original_id</th>\n",
       "      <th>original_label</th>\n",
       "      <th>original_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "      <td>{none}</td>\n",
       "      <td>not_hatespeech</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>twitter</td>\n",
       "      <td>HateXplain</td>\n",
       "      <td>1179055004553900032</td>\n",
       "      <td>normal</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "      <td>{none}</td>\n",
       "      <td>not_hatespeech</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>twitter</td>\n",
       "      <td>HateXplain</td>\n",
       "      <td>1179063826874032128</td>\n",
       "      <td>normal</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "      <td>{race}</td>\n",
       "      <td>not_hatespeech</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>twitter</td>\n",
       "      <td>HateXplain</td>\n",
       "      <td>1178793830532956161</td>\n",
       "      <td>normal</td>\n",
       "      <td>[African]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "      <td>{race}</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>hatespeech/offensive</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>twitter</td>\n",
       "      <td>HateXplain</td>\n",
       "      <td>1179088797964763136</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[Asian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "      <td>{race, gender}</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>hatespeech/offensive</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>twitter</td>\n",
       "      <td>HateXplain</td>\n",
       "      <td>1179085312976445440</td>\n",
       "      <td>hatespeech</td>\n",
       "      <td>[Caucasian, Women]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_new                                               text extracted_target  \\\n",
       "0       1  i dont think im getting my baby them white 9 h...           {none}   \n",
       "1       2  we cannot continue calling ourselves feminists...           {none}   \n",
       "2       3                      nawt yall niggers ignoring me           {race}   \n",
       "3       4  <user> i am bit confused coz chinese ppl can n...           {race}   \n",
       "4       5  this bitch in whataburger eating a burger with...   {race, gender}   \n",
       "\n",
       "  label_hatespeech_binary_offensive_not_included  \\\n",
       "0                                 not_hatespeech   \n",
       "1                                 not_hatespeech   \n",
       "2                                 not_hatespeech   \n",
       "3                                     hatespeech   \n",
       "4                                     hatespeech   \n",
       "\n",
       "  label_hatespeech_binary_offensive_included  \\\n",
       "0                                     normal   \n",
       "1                                     normal   \n",
       "2                                     normal   \n",
       "3                       hatespeech/offensive   \n",
       "4                       hatespeech/offensive   \n",
       "\n",
       "  label_normal_offensive_hatespeech platform original_dataset_title  \\\n",
       "0                            normal  twitter             HateXplain   \n",
       "1                            normal  twitter             HateXplain   \n",
       "2                            normal  twitter             HateXplain   \n",
       "3                        hatespeech  twitter             HateXplain   \n",
       "4                        hatespeech  twitter             HateXplain   \n",
       "\n",
       "           original_id original_label     original_target  \n",
       "0  1179055004553900032         normal              [None]  \n",
       "1  1179063826874032128         normal              [None]  \n",
       "2  1178793830532956161         normal           [African]  \n",
       "3  1179088797964763136     hatespeech             [Asian]  \n",
       "4  1179085312976445440     hatespeech  [Caucasian, Women]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# GitHub raw content URL for the pickle file\n",
    "github_raw_url = \"https://github.com/Takosaga/master_thesis/raw/main/data/processed/annotated_and_targeted_hatespeech.pkl\" # Updated URL\n",
    "# Download the pickle file\n",
    "response = requests.get(github_raw_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Load the pickle data into a pandas DataFrame\n",
    "    df = pd.read_pickle(BytesIO(response.content))\n",
    "    print(f\"DataFrame loaded successfully with {len(df)} rows\")\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(\"\\nPreview of the DataFrame:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41d46bc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c672b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, text_column=\"text\"):\n",
    "    \"\"\"Process an entire dataframe with efficient batching\"\"\"\n",
    "    all_texts = df[text_column].tolist()\n",
    "    total_texts = len(all_texts)\n",
    "    \n",
    "    # Get predictions in batch\n",
    "    cardiff_preds = pipe_cardiff(all_texts)\n",
    "    \n",
    "    results = []\n",
    "    for i, text in tqdm(enumerate(all_texts), total=total_texts, desc=\"Processing texts\"):\n",
    "        try:\n",
    "\n",
    "            pred_cardiff = cardiff_preds[i]\n",
    "            \n",
    "            # Calculate explanations for all examples\n",
    "            lime_exp_cardiff = lime_explain(text, predict_cardiff)\n",
    "            shap_exp_cardiff = get_shap_values(text, explainer_shap_cardiff)\n",
    "            \n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"error\": None,\n",
    "                \"cardiff_label\": pred_cardiff[\"label\"],\n",
    "                \"cardiff_score\": pred_cardiff[\"score\"],\n",
    "                \"lime_cardiff\": lime_exp_cardiff,\n",
    "                \"shap_cardiff\": shap_exp_cardiff,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"error\": f\"Caught an exception: {e}\",\n",
    "                \"cardiff_label\": None,\n",
    "                \"cardiff_score\": None,\n",
    "                \"lime_cardiff\": None,\n",
    "                \"shap_cardiff\": None,\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4432cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "section = len(df)//1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df, name, base_path='/content/drive/MyDrive/'):\n",
    "    \"\"\"\n",
    "    Save a dataframe to CSV, Excel, and pickle formats.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to save\n",
    "    - name: base name for the files (without extension)\n",
    "    - base_path: directory path where files will be saved\n",
    "    \"\"\"\n",
    "    # Save as CSV\n",
    "    df.to_csv(f'{base_path}{name}.csv', index=False)\n",
    "    \n",
    "    # Save as Excel\n",
    "    df.to_excel(f'{base_path}{name}.xlsx', index=False)\n",
    "    \n",
    "    # Save as pickle\n",
    "    df.to_pickle(f'{base_path}{name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "df_1 = process_dataframe(df[:section])\n",
    "save_dataframe(df_1, 'df_1_seed42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)\n",
    "df_1 = process_dataframe(df[:section])\n",
    "save_dataframe(df_1, 'df_1_seed0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
