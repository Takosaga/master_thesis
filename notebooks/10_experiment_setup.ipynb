{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f110a7f",
   "metadata": {},
   "source": [
    "# Setup for single factor experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d9bac",
   "metadata": {},
   "source": [
    "Using only Cardiff only and changing seeds with using LIME and SHAP default values\n",
    "\n",
    "Will only be done on google colab T4 gpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab4a77d",
   "metadata": {},
   "source": [
    "* Seed A: 0\n",
    "* Seed B: 42\n",
    "* Seed C: 123\n",
    "* Seed D: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e12cf1",
   "metadata": {},
   "source": [
    "Looking at 1% of the dataset where cardiff_score is 1.00 a quater is TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c660f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import dtale\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "\n",
    "from master_thesis.config import PROCESSED_DATA_DIR, INTERIM_DATA_DIR, load_dataframe_from_pickle, save_dataframe_as_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ba6f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>extracted_target</th>\n",
       "      <th>platform</th>\n",
       "      <th>is_hatespeech</th>\n",
       "      <th>id</th>\n",
       "      <th>cardiff_score</th>\n",
       "      <th>shap_cardiff</th>\n",
       "      <th>lime_cardiff</th>\n",
       "      <th>fb_score</th>\n",
       "      <th>shap_fb</th>\n",
       "      <th>lime_fb</th>\n",
       "      <th>cardiff_hatespeech</th>\n",
       "      <th>fb_hatespeech</th>\n",
       "      <th>lime_cardiff_wordset</th>\n",
       "      <th>lime_fb_wordset</th>\n",
       "      <th>shap_cardiff_wordset</th>\n",
       "      <th>shap_fb_wordset</th>\n",
       "      <th>jaccard_similarity_cardiff</th>\n",
       "      <th>jaccard_similarity_fb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "      <td>{none}</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992369</td>\n",
       "      <td>i : 0.002, getting : 0.001, my : 0.001, baby :...</td>\n",
       "      <td>and: 0.04, my: 0.04, white: -0.03, 9: -0.03</td>\n",
       "      <td>0.687105</td>\n",
       "      <td>: 0.014, i : 0.014, dont : -0.008, think : -0....</td>\n",
       "      <td>white: -0.19, touched: -0.12, dont: -0.11, j: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>{my, 9, white, and}</td>\n",
       "      <td>{touched, dont, j, white}</td>\n",
       "      <td>{them, has, white}</td>\n",
       "      <td>{them, not, white}</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "      <td>{none}</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.997838</td>\n",
       "      <td>calling : 0.013, ourselves : 0.014, feminists ...</td>\n",
       "      <td>trans: 0.00, gender: 0.00, addressed: 0.00, fe...</td>\n",
       "      <td>0.919145</td>\n",
       "      <td>: -0.014, we : -0.014, cannot : -0.029, contin...</td>\n",
       "      <td>and: 0.13, trans: 0.11, lesbian: 0.11, a: -0.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{addressed, trans, gender, feminists}</td>\n",
       "      <td>{lesbian, trans, a, and}</td>\n",
       "      <td>{calling, ourselves, bisexual, feminists}</td>\n",
       "      <td>{sheet, reporting, if, feminists}</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "      <td>{race}</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.935527</td>\n",
       "      <td>n: 0.010, aw: 0.006, t : 0.007, y: 0.003, all ...</td>\n",
       "      <td>niggers: -0.90, nawt: -0.09, me: 0.07, yall: 0.07</td>\n",
       "      <td>0.999480</td>\n",
       "      <td>n: 0.030, aw: -0.062, t : -0.040, y: 0.129, al...</td>\n",
       "      <td>niggers: -0.96, ignoring: -0.02, nawt: -0.02, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{nawt, niggers, yall, me}</td>\n",
       "      <td>{nawt, niggers, yall, ignoring}</td>\n",
       "      <td>{ignoring, gers, all, t}</td>\n",
       "      <td>{ignoring, gers, all, t}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "      <td>{race}</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.997049</td>\n",
       "      <td>: -0.005, &lt;: -0.005, user: -0.005, &gt; : -0.005,...</td>\n",
       "      <td>chong: -0.44, ching: -0.29, chinese: -0.08, pa...</td>\n",
       "      <td>0.779210</td>\n",
       "      <td>: 0.016, &lt;: 0.016, user: 0.016, &gt; : 0.016, i :...</td>\n",
       "      <td>ching: -0.26, this: -0.24, chong: -0.23, confu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{ching, pakistani, chong, chinese}</td>\n",
       "      <td>{ching, chong, this, confused}</td>\n",
       "      <td>{inese, ong, ing, am}</td>\n",
       "      <td>{this, inese, how, ing}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "      <td>{gender, race}</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.990916</td>\n",
       "      <td>bitch : -0.338, in : -0.145, urger : 0.007, ea...</td>\n",
       "      <td>bitches: -0.44, bitch: -0.28, hate: -0.18, eat...</td>\n",
       "      <td>0.999297</td>\n",
       "      <td>this : 0.007, bitch : -0.478, in : -0.004, wha...</td>\n",
       "      <td>bitch: -0.39, bitches: -0.30, holding: -0.05, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{bitch, eating, hate, bitches}</td>\n",
       "      <td>{i, holding, bitch, bitches}</td>\n",
       "      <td>{in, bitch, hate, white}</td>\n",
       "      <td>{bitch, burger, hate, white}</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text extracted_target  \\\n",
       "0  i dont think im getting my baby them white 9 h...           {none}   \n",
       "1  we cannot continue calling ourselves feminists...           {none}   \n",
       "2                      nawt yall niggers ignoring me           {race}   \n",
       "3  <user> i am bit confused coz chinese ppl can n...           {race}   \n",
       "4  this bitch in whataburger eating a burger with...   {gender, race}   \n",
       "\n",
       "  platform is_hatespeech  id  cardiff_score  \\\n",
       "0  twitter             0   1       0.992369   \n",
       "1  twitter             0   2       0.997838   \n",
       "2  twitter             0   3       0.935527   \n",
       "3  twitter             1   4       0.997049   \n",
       "4  twitter             1   5       0.990916   \n",
       "\n",
       "                                        shap_cardiff  \\\n",
       "0  i : 0.002, getting : 0.001, my : 0.001, baby :...   \n",
       "1  calling : 0.013, ourselves : 0.014, feminists ...   \n",
       "2  n: 0.010, aw: 0.006, t : 0.007, y: 0.003, all ...   \n",
       "3  : -0.005, <: -0.005, user: -0.005, > : -0.005,...   \n",
       "4  bitch : -0.338, in : -0.145, urger : 0.007, ea...   \n",
       "\n",
       "                                        lime_cardiff  fb_score  \\\n",
       "0        and: 0.04, my: 0.04, white: -0.03, 9: -0.03  0.687105   \n",
       "1  trans: 0.00, gender: 0.00, addressed: 0.00, fe...  0.919145   \n",
       "2  niggers: -0.90, nawt: -0.09, me: 0.07, yall: 0.07  0.999480   \n",
       "3  chong: -0.44, ching: -0.29, chinese: -0.08, pa...  0.779210   \n",
       "4  bitches: -0.44, bitch: -0.28, hate: -0.18, eat...  0.999297   \n",
       "\n",
       "                                             shap_fb  \\\n",
       "0  : 0.014, i : 0.014, dont : -0.008, think : -0....   \n",
       "1  : -0.014, we : -0.014, cannot : -0.029, contin...   \n",
       "2  n: 0.030, aw: -0.062, t : -0.040, y: 0.129, al...   \n",
       "3  : 0.016, <: 0.016, user: 0.016, > : 0.016, i :...   \n",
       "4  this : 0.007, bitch : -0.478, in : -0.004, wha...   \n",
       "\n",
       "                                             lime_fb  cardiff_hatespeech  \\\n",
       "0  white: -0.19, touched: -0.12, dont: -0.11, j: ...                   0   \n",
       "1    and: 0.13, trans: 0.11, lesbian: 0.11, a: -0.10                   0   \n",
       "2  niggers: -0.96, ignoring: -0.02, nawt: -0.02, ...                   1   \n",
       "3  ching: -0.26, this: -0.24, chong: -0.23, confu...                   1   \n",
       "4  bitch: -0.39, bitches: -0.30, holding: -0.05, ...                   1   \n",
       "\n",
       "   fb_hatespeech                   lime_cardiff_wordset  \\\n",
       "0              1                    {my, 9, white, and}   \n",
       "1              0  {addressed, trans, gender, feminists}   \n",
       "2              1              {nawt, niggers, yall, me}   \n",
       "3              1     {ching, pakistani, chong, chinese}   \n",
       "4              1         {bitch, eating, hate, bitches}   \n",
       "\n",
       "                   lime_fb_wordset                       shap_cardiff_wordset  \\\n",
       "0        {touched, dont, j, white}                         {them, has, white}   \n",
       "1         {lesbian, trans, a, and}  {calling, ourselves, bisexual, feminists}   \n",
       "2  {nawt, niggers, yall, ignoring}                   {ignoring, gers, all, t}   \n",
       "3   {ching, chong, this, confused}                      {inese, ong, ing, am}   \n",
       "4     {i, holding, bitch, bitches}                   {in, bitch, hate, white}   \n",
       "\n",
       "                     shap_fb_wordset  jaccard_similarity_cardiff  \\\n",
       "0                 {them, not, white}                    0.166667   \n",
       "1  {sheet, reporting, if, feminists}                    0.142857   \n",
       "2           {ignoring, gers, all, t}                    0.000000   \n",
       "3            {this, inese, how, ing}                    0.000000   \n",
       "4       {bitch, burger, hate, white}                    0.333333   \n",
       "\n",
       "   jaccard_similarity_fb  \n",
       "0               0.166667  \n",
       "1               0.000000  \n",
       "2               0.142857  \n",
       "3               0.142857  \n",
       "4               0.142857  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df = load_dataframe_from_pickle(\n",
    "    PROCESSED_DATA_DIR / \"models_and_XAI_with_jaccard_similarity_applied.pkl\"\n",
    ")\n",
    "\n",
    "working_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e927a85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65216, 19)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53baf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_percent = len(working_df)//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9eb8316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quarter = one_percent//4\n",
    "quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6161c0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'extracted_target',\n",
       " 'platform',\n",
       " 'is_hatespeech',\n",
       " 'id',\n",
       " 'cardiff_score',\n",
       " 'shap_cardiff',\n",
       " 'lime_cardiff',\n",
       " 'fb_score',\n",
       " 'shap_fb',\n",
       " 'lime_fb',\n",
       " 'cardiff_hatespeech',\n",
       " 'fb_hatespeech',\n",
       " 'lime_cardiff_wordset',\n",
       " 'lime_fb_wordset',\n",
       " 'shap_cardiff_wordset',\n",
       " 'shap_fb_wordset',\n",
       " 'jaccard_similarity_cardiff',\n",
       " 'jaccard_similarity_fb']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33de3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    'text', \n",
    "    'extracted_target',\n",
    "    'platform',\n",
    "    'is_hatespeech',\n",
    "    'id',\n",
    "    'cardiff_score',\n",
    "    'cardiff_hatespeech'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c34218",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = working_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10d7a76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>extracted_target</th>\n",
       "      <th>platform</th>\n",
       "      <th>is_hatespeech</th>\n",
       "      <th>id</th>\n",
       "      <th>cardiff_score</th>\n",
       "      <th>cardiff_hatespeech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont think im getting my baby them white 9 h...</td>\n",
       "      <td>{none}</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992369</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we cannot continue calling ourselves feminists...</td>\n",
       "      <td>{none}</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.997838</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "      <td>{race}</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.935527</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
       "      <td>{race}</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.997049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this bitch in whataburger eating a burger with...</td>\n",
       "      <td>{gender, race}</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.990916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text extracted_target  \\\n",
       "0  i dont think im getting my baby them white 9 h...           {none}   \n",
       "1  we cannot continue calling ourselves feminists...           {none}   \n",
       "2                      nawt yall niggers ignoring me           {race}   \n",
       "3  <user> i am bit confused coz chinese ppl can n...           {race}   \n",
       "4  this bitch in whataburger eating a burger with...   {gender, race}   \n",
       "\n",
       "  platform is_hatespeech  id  cardiff_score  cardiff_hatespeech  \n",
       "0  twitter             0   1       0.992369                   0  \n",
       "1  twitter             0   2       0.997838                   0  \n",
       "2  twitter             0   3       0.935527                   1  \n",
       "3  twitter             1   4       0.997049                   1  \n",
       "4  twitter             1   5       0.990916                   1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d236f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53395, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = working_df[\"cardiff_score\"] >= .95\n",
    "working_df_filtered = working_df[mask]\n",
    "working_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c692783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters ---\n",
    "sample_size = 163\n",
    "ground_truth_col = 'is_hatespeech'\n",
    "prediction_col = 'cardiff_hatespeech'\n",
    "\n",
    "# True Positives (TP): is_hatespeech=1, cardiff_hatespeech=1\n",
    "tp_condition = (working_df_filtered[ground_truth_col] == 1) & (working_df_filtered[prediction_col] == 1)\n",
    "df_tp = working_df_filtered[tp_condition]\n",
    "\n",
    "# True Negatives (TN): is_hatespeech=0, cardiff_hatespeech=0\n",
    "tn_condition = (working_df_filtered[ground_truth_col] == 0) & (working_df_filtered[prediction_col] == 0)\n",
    "df_tn = working_df_filtered[tn_condition]\n",
    "\n",
    "# False Positives (FP): is_hatespeech=0, cardiff_hatespeech=1\n",
    "fp_condition = (working_df_filtered[ground_truth_col] == 0) & (working_df_filtered[prediction_col] == 1)\n",
    "df_fp = working_df_filtered[fp_condition]\n",
    "\n",
    "# False Negatives (FN): is_hatespeech=1, cardiff_hatespeech=0\n",
    "fn_condition = (working_df_filtered[ground_truth_col] == 1) & (working_df_filtered[prediction_col] == 0)\n",
    "df_fn = working_df_filtered[fn_condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76cba524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of True Positives (TP): 16415\n",
      "Number of True Negatives (TN): 24929\n",
      "Number of False Positives (FP): 6516\n",
      "Number of False Negatives (FN): 5535\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nNumber of True Positives (TP): {len(df_tp)}\")\n",
    "print(f\"Number of True Negatives (TN): {len(df_tn)}\")\n",
    "print(f\"Number of False Positives (FP): {len(df_fp)}\")\n",
    "print(f\"Number of False Negatives (FN): {len(df_fn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06ad00fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample and add to list\n",
    "def sample_category(df_category, category_name, num_samples):\n",
    "    if len(df_category) >= num_samples:\n",
    "        sampled_df = df_category.sample(n=num_samples, random_state=42) # random_state for reproducibility\n",
    "    else:\n",
    "        print(f\"Warning: Category '{category_name}' has only {len(df_category)} samples, which is less than {num_samples}. Taking all available samples.\")\n",
    "        sampled_df = df_category.copy() # Take all if less than desired\n",
    "    print(f\"Sampled {len(sampled_df)} from {category_name}\")\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d52464ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 163 from True Positives\n",
      "Sampled 163 from True Negatives\n",
      "Sampled 163 from False Positives\n",
      "Sampled 163 from False Negatives\n"
     ]
    }
   ],
   "source": [
    "sampled_dfs = []\n",
    "\n",
    "sampled_tp = sample_category(df_tp, 'True Positives', quarter)\n",
    "sampled_tn = sample_category(df_tn, 'True Negatives', quarter)\n",
    "sampled_fp = sample_category(df_fp, 'False Positives', quarter)\n",
    "sampled_fn = sample_category(df_fn, 'False Negatives', quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8a0018e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(652, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sampled_df = pd.concat([sampled_tp, sampled_tn, sampled_fp, sampled_fn], ignore_index=True)\n",
    "final_sampled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77384ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>extracted_target</th>\n",
       "      <th>platform</th>\n",
       "      <th>is_hatespeech</th>\n",
       "      <th>id</th>\n",
       "      <th>cardiff_score</th>\n",
       "      <th>cardiff_hatespeech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India is one nasty, dirty, filthy, germsy, law...</td>\n",
       "      <td>{nationality}</td>\n",
       "      <td>youtube</td>\n",
       "      <td>1</td>\n",
       "      <td>53513</td>\n",
       "      <td>0.994301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>solo que seras bombardeas y destruidos como lo...</td>\n",
       "      <td>{religion}</td>\n",
       "      <td>gab</td>\n",
       "      <td>1</td>\n",
       "      <td>8335</td>\n",
       "      <td>0.957913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user You cannot argue with a retard jer...</td>\n",
       "      <td>{disability, nationality}</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>64112</td>\n",
       "      <td>0.994864</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can I just pound that pussy an pull your hair....</td>\n",
       "      <td>{gender}</td>\n",
       "      <td>reddit</td>\n",
       "      <td>1</td>\n",
       "      <td>22034</td>\n",
       "      <td>0.977588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you do not live in america nazi faggot but you...</td>\n",
       "      <td>{sexuality}</td>\n",
       "      <td>gab</td>\n",
       "      <td>1</td>\n",
       "      <td>18143</td>\n",
       "      <td>0.985650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  India is one nasty, dirty, filthy, germsy, law...   \n",
       "1  solo que seras bombardeas y destruidos como lo...   \n",
       "2  @user @user You cannot argue with a retard jer...   \n",
       "3  Can I just pound that pussy an pull your hair....   \n",
       "4  you do not live in america nazi faggot but you...   \n",
       "\n",
       "            extracted_target platform is_hatespeech     id  cardiff_score  \\\n",
       "0              {nationality}  youtube             1  53513       0.994301   \n",
       "1                 {religion}      gab             1   8335       0.957913   \n",
       "2  {disability, nationality}  twitter             1  64112       0.994864   \n",
       "3                   {gender}   reddit             1  22034       0.977588   \n",
       "4                {sexuality}      gab             1  18143       0.985650   \n",
       "\n",
       "   cardiff_hatespeech  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df91210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to /home/takosaga/Projects/master_thesis/data/processed/experiement_samples.pkl\n"
     ]
    }
   ],
   "source": [
    "save_dataframe_as_pickle(final_sampled_df, PROCESSED_DATA_DIR.as_posix() + '/experiement_samples.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33788a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sampled_df.to_csv(PROCESSED_DATA_DIR.as_posix() + '/experiement_samples.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a65a6",
   "metadata": {},
   "source": [
    "## Setup for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install lime shap hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity test\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import random\n",
    "import torch\n",
    "from typing import List, Union\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        # Optional: for determinism with CuDNN\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Constants\n",
    "CLASS_NAMES = ['NOT-HATE', 'HATE']\n",
    "BATCH_SIZE = 512  # Adjust based on your GPU memory\n",
    "\n",
    "# Load models and tokenizers once\n",
    "pipe_cardiff = pipeline(\"text-classification\", \n",
    "                       model=\"cardiffnlp/twitter-roberta-base-hate-latest\", \n",
    "                       device=0 if torch.cuda.is_available() else -1,\n",
    "                       batch_size=BATCH_SIZE)\n",
    "\n",
    "tokenizer_cardiff = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-hate-latest\")\n",
    "\n",
    "# Create dataset class for more efficient processing\n",
    "class HateSpeechDataset:\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "\n",
    "# Unified prediction function for both models\n",
    "def batch_predict(texts: Union[List[str], np.ndarray], pipeline_fn):\n",
    "    \"\"\"Run predictions in batches for efficiency\"\"\"\n",
    "    if isinstance(texts, np.ndarray):\n",
    "        texts = texts.tolist()\n",
    "        \n",
    "    dataset = HateSpeechDataset(texts)\n",
    "    \n",
    "    # Process in batches\n",
    "    all_outputs = []\n",
    "    for i in range(0, len(dataset), BATCH_SIZE):\n",
    "        batch_texts = [dataset[j] for j in range(i, min(i + BATCH_SIZE, len(dataset)))]\n",
    "        outputs = pipeline_fn(batch_texts, top_k=2)\n",
    "        all_outputs.extend(outputs)\n",
    "    \n",
    "    # Convert to numpy array with consistent order\n",
    "    return np.array([\n",
    "        [label['score'] for label in sorted(res, key=lambda x: x['label'])]\n",
    "        for res in all_outputs\n",
    "    ])\n",
    "\n",
    "\n",
    "# Model-specific prediction functions\n",
    "def predict_cardiff(texts):\n",
    "    return batch_predict(texts, pipe_cardiff)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize explainers\n",
    "explainer_lime = LimeTextExplainer(class_names=CLASS_NAMES)\n",
    "explainer_shap_cardiff = shap.Explainer(predict_cardiff, \n",
    "                                       masker=shap.maskers.Text(tokenizer_cardiff))\n",
    "\n",
    "# Helper function for LIME explanations\n",
    "def lime_explain(text, predictor, num_features=4):\n",
    "    exp = explainer_lime.explain_instance(text, \n",
    "                                          predictor)\n",
    "    return \", \".join([f\"{word}: {weight}\" for word, weight in exp.as_list()])\n",
    "\n",
    "\n",
    "# Helper function for SHAP explanations\n",
    "def get_shap_values(text, explainer, class_idx=1):\n",
    "    shap_vals = explainer([text])\n",
    "    return \", \".join([\n",
    "        f\"{feature}: {value}\"\n",
    "        for feature, value in zip(shap_vals[0].data, shap_vals[0].values[:, class_idx])\n",
    "        if abs(value) != 0\n",
    "    ])\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'text': [\n",
    "        \"nawt yall niggers ignoring me\",\n",
    "        \"and this is why i end up with nigger trainee doctors who can not speak properly lack basic knowledge of biology it truly scary if the public only knew\",\n",
    "        \"this is a neutral sentence\"\n",
    "    ]\n",
    "}\n",
    "annotated_and_targeted_hatespeech_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Process all texts in one batch for efficiency\n",
    "all_texts = annotated_and_targeted_hatespeech_df[\"text\"].tolist()\n",
    "\n",
    "# Get predictions in batch\n",
    "cardiff_preds = pipe_cardiff(all_texts) \n",
    "\n",
    "# Process results\n",
    "results = []\n",
    "for i, text in enumerate(all_texts):\n",
    "    try:\n",
    "        # Get predictions from the batch results\n",
    "        pred_cardiff = cardiff_preds[i]\n",
    "\n",
    "        # LIME explanations\n",
    "        lime_exp_cardiff = lime_explain(text, predict_cardiff)\n",
    "\n",
    "        # SHAP explanations\n",
    "        shap_exp_cardiff = get_shap_values(text, explainer_shap_cardiff)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"error\": None,\n",
    "            \"cardiff_label\": pred_cardiff[\"label\"],\n",
    "            \"cardiff_score\": pred_cardiff[\"score\"],\n",
    "            \"lime_cardiff\": lime_exp_cardiff,\n",
    "            \"shap_cardiff\": shap_exp_cardiff\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"error\": f\"Caught an exception: {e}\",\n",
    "            \"cardiff_label\": None,\n",
    "            \"cardiff_score\": None,\n",
    "            \"lime_cardiff\": None,\n",
    "            \"shap_cardiff\": None\n",
    "        })\n",
    "\n",
    "final_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41244756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Save to your Google Drive\n",
    "final_df.to_csv('/content/drive/MyDrive/hate_speech_analysis_results.csv', index=False)\n",
    "final_df.to_excel('/content/drive/MyDrive/hate_speech_analysis_results.xlsx', index=False)\n",
    "final_df.to_pickle('/content/drive/MyDrive/hate_speech_analysis_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4baeeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# GitHub raw content URL for the pickle file\n",
    "github_raw_url = \"https://github.com/Takosaga/master_thesis/raw/main/data/processed/experiement_samples.pkl\" \n",
    "# Download the pickle file\n",
    "response = requests.get(github_raw_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Load the pickle data into a pandas DataFrame\n",
    "    df = pd.read_pickle(BytesIO(response.content))\n",
    "    print(f\"DataFrame loaded successfully with {len(df)} rows\")\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(\"\\nPreview of the DataFrame:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c672b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, seed, text_column=\"text\"):\n",
    "    \"\"\"Process an entire dataframe with efficient batching\"\"\"\n",
    "    all_texts = df[text_column].tolist()\n",
    "    total_texts = len(all_texts)\n",
    "    \n",
    "    # Get predictions in batch\n",
    "    cardiff_preds = pipe_cardiff(all_texts)\n",
    "    \n",
    "    results = []\n",
    "    for i, text in tqdm(enumerate(all_texts), total=total_texts, desc=\"Processing texts\"):\n",
    "        try:\n",
    "            set_seed(seed)\n",
    "            pred_cardiff = cardiff_preds[i]\n",
    "            \n",
    "            # Calculate explanations for all examples and setting seed\n",
    "            set_seed(seed)\n",
    "            lime_exp_cardiff = lime_explain(text, predict_cardiff)\n",
    "            set_seed(seed)\n",
    "            shap_exp_cardiff = get_shap_values(text, explainer_shap_cardiff)\n",
    "            \n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"error\": None,\n",
    "                \"cardiff_label\": pred_cardiff[\"label\"],\n",
    "                \"cardiff_score\": pred_cardiff[\"score\"],\n",
    "                \"lime_cardiff\": lime_exp_cardiff,\n",
    "                \"shap_cardiff\": shap_exp_cardiff,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"error\": f\"Caught an exception: {e}\",\n",
    "                \"cardiff_label\": None,\n",
    "                \"cardiff_score\": None,\n",
    "                \"lime_cardiff\": None,\n",
    "                \"shap_cardiff\": None,\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df, name, base_path='/content/drive/MyDrive/'):\n",
    "    \"\"\"\n",
    "    Save a dataframe to CSV, Excel, and pickle formats.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to save\n",
    "    - name: base name for the files (without extension)\n",
    "    - base_path: directory path where files will be saved\n",
    "    \"\"\"\n",
    "    # Save as CSV\n",
    "    df.to_csv(f'{base_path}{name}.csv', index=False)\n",
    "    \n",
    "    # Save as Excel\n",
    "    df.to_excel(f'{base_path}{name}.xlsx', index=False)\n",
    "    \n",
    "    # Save as pickle\n",
    "    df.to_pickle(f'{base_path}{name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed A: Using samples with seed 0\n",
    "df_seed_0 = process_dataframe(df, 0)\n",
    "save_dataframe(df_seed_0, 'df_seed_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed B: Using samples with seed 42\n",
    "df_seed_42 = process_dataframe(df, 42)\n",
    "save_dataframe(df_seed_42, 'df_seed_42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbabd66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed C: Using samples with seed 123\n",
    "df_seed_123 = process_dataframe(df, 123)\n",
    "save_dataframe(df_seed_123, 'df_seed_123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b453db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed D: Using samples with seed 2025\n",
    "df_seed_2025 = process_dataframe(df, 2025)\n",
    "save_dataframe(df_seed_2025, 'df_seed_2025')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a325a07",
   "metadata": {},
   "source": [
    "### Combining Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2972f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "# Loop through seeds\n",
    "for i in [0, 42, 123, 2025]:\n",
    "    file_path = INTERIM_DATA_DIR / f\"df_seed_{i}.csv\"\n",
    "    if file_path.exists():  # Check if file exists\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Optionally add a source column to track which file each row came from\n",
    "        df['seed'] = i\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: {file_path} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75cab216",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b8d04ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (2608, 7)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Combined DataFrame shape: {combined_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c892e7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to /home/takosaga/Projects/master_thesis/data/processed/samples_with_XAI_applied.pkl\n"
     ]
    }
   ],
   "source": [
    "save_dataframe_as_pickle(combined_df, PROCESSED_DATA_DIR.as_posix() + '/samples_with_XAI_applied.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77804ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(PROCESSED_DATA_DIR.as_posix() + '/samples_with_XAI_applied.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
