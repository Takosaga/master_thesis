{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6da5e4",
   "metadata": {},
   "source": [
    "# Full Experiment Rerun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f38035",
   "metadata": {},
   "source": [
    "Making a full rerun with LIME and SHAP with seeds and random state in the Explainer to be set along with \n",
    "\n",
    "Initial runs had the following to reset seed\n",
    "```\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        # Optional: for determinism with CuDNN\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35acc66d",
   "metadata": {},
   "source": [
    "## Setup for Google Colab\n",
    "\n",
    "Ran on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install lime shap hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7fa2313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 15:18:34.316125: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-11 15:18:34.507816: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-11 15:18:34.507850: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-11 15:18:34.508462: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-11 15:18:34.608454: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-11 15:18:35.526549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49d0df5d27d433a93d57daefef2ff3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>error</th>\n",
       "      <th>cardiff_label</th>\n",
       "      <th>cardiff_score</th>\n",
       "      <th>lime_cardiff</th>\n",
       "      <th>shap_cardiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nawt yall niggers ignoring me</td>\n",
       "      <td>None</td>\n",
       "      <td>HATE</td>\n",
       "      <td>0.935527</td>\n",
       "      <td>niggers: -0.9069088206585199, nawt: -0.1023075...</td>\n",
       "      <td>: -4.889443516731262e-08, n: 0.009972625062800...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and this is why i end up with nigger trainee d...</td>\n",
       "      <td>None</td>\n",
       "      <td>HATE</td>\n",
       "      <td>0.993975</td>\n",
       "      <td>nigger: -0.9810312019844282, with: -0.00693745...</td>\n",
       "      <td>: 1.1641532182693481e-10, and : 0.002760913455...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is a neutral sentence</td>\n",
       "      <td>None</td>\n",
       "      <td>NOT-HATE</td>\n",
       "      <td>0.999157</td>\n",
       "      <td>a: 0.0007495249764967522, is: 0.00067701660540...</td>\n",
       "      <td>this : 0.0007638931274414062, is : 0.000668436...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text error cardiff_label  \\\n",
       "0                      nawt yall niggers ignoring me  None          HATE   \n",
       "1  and this is why i end up with nigger trainee d...  None          HATE   \n",
       "2                         this is a neutral sentence  None      NOT-HATE   \n",
       "\n",
       "   cardiff_score                                       lime_cardiff  \\\n",
       "0       0.935527  niggers: -0.9069088206585199, nawt: -0.1023075...   \n",
       "1       0.993975  nigger: -0.9810312019844282, with: -0.00693745...   \n",
       "2       0.999157  a: 0.0007495249764967522, is: 0.00067701660540...   \n",
       "\n",
       "                                        shap_cardiff  \n",
       "0  : -4.889443516731262e-08, n: 0.009972625062800...  \n",
       "1  : 1.1641532182693481e-10, and : 0.002760913455...  \n",
       "2  this : 0.0007638931274414062, is : 0.000668436...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity test\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import random\n",
    "import torch\n",
    "from typing import List, Union\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        # Optional: for determinism with CuDNN\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 512  # Adjust based on your GPU memory\n",
    "\n",
    "# Load models and tokenizers once\n",
    "pipe_cardiff = pipeline(\"text-classification\", \n",
    "                       model=\"cardiffnlp/twitter-roberta-base-hate-latest\", \n",
    "                       device=0 if torch.cuda.is_available() else -1,\n",
    "                       batch_size=BATCH_SIZE)\n",
    "\n",
    "tokenizer_cardiff = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-hate-latest\")\n",
    "\n",
    "# Create dataset class for more efficient processing\n",
    "class HateSpeechDataset:\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "\n",
    "# Unified prediction function for both models\n",
    "def batch_predict(texts: Union[List[str], np.ndarray], pipeline_fn):\n",
    "    \"\"\"Run predictions in batches for efficiency\"\"\"\n",
    "    if isinstance(texts, np.ndarray):\n",
    "        texts = texts.tolist()\n",
    "        \n",
    "    dataset = HateSpeechDataset(texts)\n",
    "    \n",
    "    # Process in batches\n",
    "    all_outputs = []\n",
    "    for i in range(0, len(dataset), BATCH_SIZE):\n",
    "        batch_texts = [dataset[j] for j in range(i, min(i + BATCH_SIZE, len(dataset)))]\n",
    "        outputs = pipeline_fn(batch_texts, top_k=2)\n",
    "        all_outputs.extend(outputs)\n",
    "    \n",
    "    # Convert to numpy array with consistent order\n",
    "    return np.array([\n",
    "        [label['score'] for label in sorted(res, key=lambda x: x['label'])]\n",
    "        for res in all_outputs\n",
    "    ])\n",
    "\n",
    "\n",
    "# Model-specific prediction functions\n",
    "def predict_cardiff(texts):\n",
    "    return batch_predict(texts, pipe_cardiff)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize explainers\n",
    "explainer_lime = LimeTextExplainer()\n",
    "explainer_shap_cardiff = shap.Explainer(predict_cardiff, \n",
    "                                       masker=shap.maskers.Text(tokenizer_cardiff))\n",
    "\n",
    "# Helper function for LIME explanations\n",
    "def lime_explain(text, predictor):\n",
    "    exp = explainer_lime.explain_instance(text, \n",
    "                                          predictor)\n",
    "    return \", \".join([f\"{word}: {weight}\" for word, weight in exp.as_list()])\n",
    "\n",
    "\n",
    "# Helper function for SHAP explanations\n",
    "def get_shap_values(text, explainer, class_idx=1):\n",
    "    shap_vals = explainer([text])\n",
    "    return \", \".join([\n",
    "        f\"{feature}: {value}\"\n",
    "        for feature, value in zip(shap_vals[0].data, shap_vals[0].values[:, class_idx])\n",
    "        if abs(value) != 0\n",
    "    ])\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'text': [\n",
    "        \"nawt yall niggers ignoring me\",\n",
    "        \"and this is why i end up with nigger trainee doctors who can not speak properly lack basic knowledge of biology it truly scary if the public only knew\",\n",
    "        \"this is a neutral sentence\"\n",
    "    ]\n",
    "}\n",
    "annotated_and_targeted_hatespeech_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Process all texts in one batch for efficiency\n",
    "all_texts = annotated_and_targeted_hatespeech_df[\"text\"].tolist()\n",
    "\n",
    "# Get predictions in batch\n",
    "cardiff_preds = pipe_cardiff(all_texts) \n",
    "\n",
    "# Process results\n",
    "results = []\n",
    "for i, text in enumerate(all_texts):\n",
    "    try:\n",
    "        # Get predictions from the batch results\n",
    "        pred_cardiff = cardiff_preds[i]\n",
    "\n",
    "        # LIME explanations\n",
    "        lime_exp_cardiff = lime_explain(text, predict_cardiff)\n",
    "\n",
    "        # SHAP explanations\n",
    "        shap_exp_cardiff = get_shap_values(text, explainer_shap_cardiff)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"error\": None,\n",
    "            \"cardiff_label\": pred_cardiff[\"label\"],\n",
    "            \"cardiff_score\": pred_cardiff[\"score\"],\n",
    "            \"lime_cardiff\": lime_exp_cardiff,\n",
    "            \"shap_cardiff\": shap_exp_cardiff\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"error\": f\"Caught an exception: {e}\",\n",
    "            \"cardiff_label\": None,\n",
    "            \"cardiff_score\": None,\n",
    "            \"lime_cardiff\": None,\n",
    "            \"shap_cardiff\": None\n",
    "        })\n",
    "\n",
    "final_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae26cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Save to your Google Drive\n",
    "final_df.to_csv('/content/drive/MyDrive/hate_speech_analysis_results.csv', index=False)\n",
    "final_df.to_excel('/content/drive/MyDrive/hate_speech_analysis_results.xlsx', index=False)\n",
    "final_df.to_pickle('/content/drive/MyDrive/hate_speech_analysis_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac57a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded successfully with 652 rows\n",
      "\n",
      "Preview of the DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>extracted_target</th>\n",
       "      <th>platform</th>\n",
       "      <th>is_hatespeech</th>\n",
       "      <th>id</th>\n",
       "      <th>cardiff_score</th>\n",
       "      <th>cardiff_hatespeech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India is one nasty, dirty, filthy, germsy, law...</td>\n",
       "      <td>{nationality}</td>\n",
       "      <td>youtube</td>\n",
       "      <td>1</td>\n",
       "      <td>53513</td>\n",
       "      <td>0.994301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>solo que seras bombardeas y destruidos como lo...</td>\n",
       "      <td>{religion}</td>\n",
       "      <td>gab</td>\n",
       "      <td>1</td>\n",
       "      <td>8335</td>\n",
       "      <td>0.957913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user You cannot argue with a retard jer...</td>\n",
       "      <td>{disability, nationality}</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>64112</td>\n",
       "      <td>0.994864</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can I just pound that pussy an pull your hair....</td>\n",
       "      <td>{gender}</td>\n",
       "      <td>reddit</td>\n",
       "      <td>1</td>\n",
       "      <td>22034</td>\n",
       "      <td>0.977588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you do not live in america nazi faggot but you...</td>\n",
       "      <td>{sexuality}</td>\n",
       "      <td>gab</td>\n",
       "      <td>1</td>\n",
       "      <td>18143</td>\n",
       "      <td>0.985650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  India is one nasty, dirty, filthy, germsy, law...   \n",
       "1  solo que seras bombardeas y destruidos como lo...   \n",
       "2  @user @user You cannot argue with a retard jer...   \n",
       "3  Can I just pound that pussy an pull your hair....   \n",
       "4  you do not live in america nazi faggot but you...   \n",
       "\n",
       "            extracted_target platform is_hatespeech     id  cardiff_score  \\\n",
       "0              {nationality}  youtube             1  53513       0.994301   \n",
       "1                 {religion}      gab             1   8335       0.957913   \n",
       "2  {disability, nationality}  twitter             1  64112       0.994864   \n",
       "3                   {gender}   reddit             1  22034       0.977588   \n",
       "4                {sexuality}      gab             1  18143       0.985650   \n",
       "\n",
       "   cardiff_hatespeech  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# GitHub raw content URL for the pickle file\n",
    "github_raw_url = \"https://github.com/Takosaga/master_thesis/raw/main/data/processed/experiement_samples.pkl\" \n",
    "# Download the pickle file\n",
    "response = requests.get(github_raw_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Load the pickle data into a pandas DataFrame\n",
    "    df = pd.read_pickle(BytesIO(response.content))\n",
    "    print(f\"DataFrame loaded successfully with {len(df)} rows\")\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(\"\\nPreview of the DataFrame:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf18733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, seed, text_column=\"text\"):\n",
    "    \"\"\"Process an entire dataframe with efficient batching\"\"\"\n",
    "    all_texts = df[text_column].tolist()\n",
    "    total_texts = len(all_texts)\n",
    "    \n",
    "    # Get predictions in batch\n",
    "    cardiff_preds = pipe_cardiff(all_texts)\n",
    "    \n",
    "    results = []\n",
    "    for i, text in tqdm(enumerate(all_texts), total=total_texts, desc=\"Processing texts\"):\n",
    "        try:\n",
    "            set_seed(seed)\n",
    "            pred_cardiff = cardiff_preds[i]\n",
    "            \n",
    "            # Calculate explanations for all examples and setting seed\n",
    "            set_seed(seed)\n",
    "            lime_exp_cardiff = lime_explain(text, predict_cardiff,random_state=seed)\n",
    "            set_seed(seed)\n",
    "            shap_exp_cardiff = get_shap_values(text, explainer_shap_cardiff, seed=seed)\n",
    "            \n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"error\": None,\n",
    "                \"cardiff_label\": pred_cardiff[\"label\"],\n",
    "                \"cardiff_score\": pred_cardiff[\"score\"],\n",
    "                \"lime_cardiff\": lime_exp_cardiff,\n",
    "                \"shap_cardiff\": shap_exp_cardiff,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"error\": f\"Caught an exception: {e}\",\n",
    "                \"cardiff_label\": None,\n",
    "                \"cardiff_score\": None,\n",
    "                \"lime_cardiff\": None,\n",
    "                \"shap_cardiff\": None,\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e2f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df, name, base_path='/content/drive/MyDrive/'):\n",
    "    \"\"\"\n",
    "    Save a dataframe to CSV, Excel, and pickle formats.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to save\n",
    "    - name: base name for the files (without extension)\n",
    "    - base_path: directory path where files will be saved\n",
    "    \"\"\"\n",
    "    # Save as CSV\n",
    "    df.to_csv(f'{base_path}{name}.csv', index=False)\n",
    "    \n",
    "    # Save as Excel\n",
    "    df.to_excel(f'{base_path}{name}.xlsx', index=False)\n",
    "    \n",
    "    # Save as pickle\n",
    "    df.to_pickle(f'{base_path}{name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1be9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seed_0 = process_dataframe(df, 0)\n",
    "save_dataframe(df_seed_0, 'df_rerun_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seed_42 = process_dataframe(df, 42)\n",
    "save_dataframe(df_seed_42, 'df_rerun_42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e73fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seed_123 = process_dataframe(df, 123)\n",
    "save_dataframe(df_seed_123, 'df_rerun_123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seed_2025 = process_dataframe(df, 2025)\n",
    "save_dataframe(df_seed_2025, 'df_rerun_2025')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
