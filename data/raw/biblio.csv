"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"CLG2ANVA","journalArticle","2021","Ayoub, J.; Yang, X.J.; Zhou, F.","Combat COVID-19 infodemic using explainable natural language processing models","Information Processing and Management","","03064573 (ISSN)","10.1016/j.ipm.2021.102569","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102851698&doi=10.1016%2fj.ipm.2021.102569&partnerID=40&md5=947a43e0ab4d3fcfbca3b0d3245f8461","Misinformation of COVID-19 is prevalent on social media as the pandemic unfolds, and the associated risks are extremely high. Thus, it is critical to detect and combat such misinformation. Recently, deep learning models using natural language processing techniques, such as BERT (Bidirectional Encoder Representations from Transformers), have achieved great successes in detecting misinformation. In this paper, we proposed an explainable natural language processing model based on DistilBERT and SHAP (Shapley Additive exPlanations) to combat misinformation about COVID-19 due to their efficiency and effectiveness. First, we collected a dataset of 984 claims about COVID-19 with fact-checking. By augmenting the data using back-translation, we doubled the sample size of the dataset and the DistilBERT model was able to obtain good performance (accuracy: 0.972; areas under the curve: 0.993) in detecting misinformation about COVID-19. Our model was also tested on a larger dataset for AAAI2021 — COVID-19 Fake News Detection Shared Task and obtained good performance (accuracy: 0.938; areas under the curve: 0.985). The performance on both datasets was better than traditional machine learning models. Second, in order to boost public trust in model prediction, we employed SHAP to improve model explainability, which was further evaluated using a between-subjects experiment with three conditions, i.e., text (T), text+SHAP explanation (TSE), and text+SHAP explanation+source and evidence (TSESE). The participants were significantly more likely to trust and share information related to COVID-19 in the TSE and TSESE conditions than in the T condition. Our results provided good implications for detecting misinformation about COVID-19 and improving public trust. © 2021 Elsevier Ltd","2021","2024-12-01 10:11:32","2024-12-14 18:55:00","","","","4","58","","Inf. Process. Manage.","","","","","","","","English","","","Scopus","","","","Publisher: Elsevier Ltd","","","","Deep learning; Learning algorithms; Natural language processing systems; BERT; Bidirectional encoder representation from transformer; Condition; COVID-19; Distilbert; DistilBERT; Misinformation detection; Natural languages; Performance; SHAP; Shapley; Shapley additive explanation; Trust; manual_BERT; manual_SHAP; manual_misinformation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KT8UUNHH","conferencePaper","2021","Tafannum, F.; Sharear Shopnil, M.N.; Salsabil, A.; Ahmed, N.; Rabiul Alam, M.G.; Tanzim Reza, M.","Demystifying Black-box Learning Models of Rumor Detection from Social Media Posts","IEEE Annu. Ubiquitous Comput., Electron. Mob. Commun. Conf., UEMCON","978-166540690-1 (ISBN)","","10.1109/UEMCON53757.2021.9666567","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125191948&doi=10.1109%2fUEMCON53757.2021.9666567&partnerID=40&md5=983ff74978be14c93e909ebb35f2585a","Social media and its users are vulnerable to the spread of rumors, therefore, protecting users from the spread of rumors is extremely important. For this reason, we propose a novel approach for rumor detection in social media that consists of multiple robust models: XGBoost Classifier, Support Vector Machine, Random Forest Classifier, Extra Tree Classifier, Decision Tree Classifier, a hybrid model, deep learning models-LSTM and BERT. For evaluation, two datasets are used. These artificial intelligence algorithms are often referred to as ""Blackbox""where data go in the box and predictions come out of the box but what is happening inside the box frequently remains cloudy. Although, there have been several works on detecting fake news, the number of works regarding rumor detection is still limited and the models used in the existing works do not explain their decision-making process. We take models with higher accuracy to illustrate which feature of the data contributes the most for a post to have been predicted as a rumor or a non-rumor by the models to explain the opaque process happening inside the black-box models. Our hybrid model achieves an accuracy of 93.22% and 82.49%, while LSTM provides 99.81%, 98.41% and BERT provides 99.62%, 94.80% accuracy scores on the COVID19 Fake News and the concatenation of Twitter15 and Twitter16 datasets respectively.  © 2021 IEEE.","2021","2024-12-01 10:11:32","2024-12-14 19:11:37","","358-364","","","","","","","","","","","Institute of Electrical and Electronics Engineers Inc.","","English","","","Scopus","","","","Journal Abbreviation: IEEE Annu. Ubiquitous Comput., Electron. Mob. Commun. Conf., UEMCON","","","","Social media; Machine-learning; Deep learning; Learning models; Social networking (online); Deep Learning; Long short-term memory; Fake detection; Black boxes; Black-box; Decision trees; Detection; Explainable; Hybrid model; Machine Learning; Random forests; Robust modeling; Rumor; Support vector machines; manual_LSTM; manual_LIME; manual_troll","","Paul R.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2021 IEEE 12th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2021","","","","","","","","","","","","","","",""
"K8H8F8KB","journalArticle","2022","Ahmed, M.; Hossain, M.S.; Ul Islam, R.; Andersson, K.","Explainable Text Classification Model for COVID-19 Fake News Detection","Journal of Internet Services and Information Security","","21822069 (ISSN)","10.22667/JISIS.2022.05.31.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132973856&doi=10.22667%2fJISIS.2022.05.31.051&partnerID=40&md5=2214ddcd129a1e25a70c76417e50e0e8","Artificial intelligence has achieved notable advances across many applications, and the field is recently concerned with developing novel methods to explain machine learning models. Deep neural networks deliver the best performance accuracy in different domains, such as text categorization, image classification, and speech recognition. Since the neural network models are black-box types, they lack transparency and explainability in predicting results. During the COVID-19 pandemic, Fake News Detection is a challenging research problem as it endangers the lives of many online users by providing misinformation. Therefore, the transparency and explainability of COVID-19 fake news classification are necessary for building the trustworthiness of model prediction. We proposed an integrated LIME-BiLSTM model where BiLSTM assures classification accuracy, and LIME ensures transparency and explainability. In this integrated model, since LIME behaves similarly to the original model and explains the prediction, the proposed model becomes comprehensible. The performance of this model in terms of explainability is measured by using Kendall’s tau correlation coefficient. We also employ several machine learning models and provide a comparison of their performances. Therefore, we analyzed and compared the computation overhead of our proposed model with the other methods because the model takes the integrated strategy. © 2022, Innovative Information Science and Technology Research Group. All rights reserved.","2022","2024-12-01 10:11:31","2024-12-14 19:54:21","","51-69","","2","12","","J. Int. Ser. Info. Sec.","","","","","","","","English","","","Scopus","","","","Publisher: Innovative Information Science and Technology Research Group","","","","Explainable AI; fake news; LIME; COVID-19; BiLSTM; manual_LSTM; manual_LIME; manual_fake_news","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5KJFW4EU","conferencePaper","2022","Nandini, D.; Schmid, U.","Explaining Hate Speech Classification with Model-Agnostic Methods","CEUR Workshop Proc.","16130073 (ISSN)","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171267300&partnerID=40&md5=58be35ae3e2ba6f423450275399f0309","There have been remarkable breakthroughs in Machine Learning (ML) and Artificial Intelligence (AI), notably in the areas of Natural Language Processing (NLP) and Deep Learning. Additionally, hate speech detection in dialogues has been gaining popularity among Natural Language Processing researchers with the increased use of social media. However, as evidenced by the recent trends, the need for the dimensions of explainability and interpretability in AI models has been deeply realised. Taking note of the factors above, the research goal of this paper is to bridge the gap between hate speech prediction and the explanations generated by the system to support its decision. This has been achieved by first predicting the classification of a text and then providing a post-hoc, model-agnostic and surrogate interpretability approach for explainability and to prevent model bias. The bidirectional transformer model BERT has been used for prediction because of its state-of-the-art efficiency over other Machine Learning(ML) models. The model-agnostic algorithm LIME generates explanations for the output of a trained classifier and predicts the features that influence the model’s decision. The predictions generated from the model were evaluated manually, and after thorough evaluation, we observed that the model performs efficiently in predicting and explaining its prediction. Lastly, we suggest further directions for the expansion of the provided research work. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","2022","2024-12-01 10:11:31","2024-12-14 19:55:33","","","","","3457","","","","","","","","CEUR-WS","","English","","","Scopus","","","","Journal Abbreviation: CEUR Workshop Proc.","","","","Interpretable artificial intelligence; Learning systems; Machine-learning; Deep learning; Forecasting; Lime; Interpretability; Natural language processing systems; Speech recognition; Natural languages; Language processing; Speech classification; Text processing; Hate speech; Speech detection; Interpretable AI; Model-agnostic method; Model-agnostic Methods; Post-hoc explainaibility; Post-hoc Explainaibility; manual_BERT; manual_LIME; manual_hate","","Koert D.; Technical Univerity Darmstadt, Department of Computer Science, Hochschulstrasse 10, Darmstadt; Minor M.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CEUR Workshop Proceedings","","","","","","","","","","","","","","",""
"9PNYQ5WI","conferencePaper","2022","Babaeianjelodar, M.; Poorna Prudhvi, G.; Lorenz, S.; Chen, K.; Mondal, S.; Dey, S.; Kumar, N.","Interpretable and High-Performance Hate and Offensive Speech Detection","Lect. Notes Comput. Sci.","03029743 (ISSN); 978-303121706-7 (ISBN)","","10.1007/978-3-031-21707-4_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144390230&doi=10.1007%2f978-3-031-21707-4_18&partnerID=40&md5=d06d016492a48955881e7f332bc27414","The spread of information through social media platforms can create environments possibly hostile to vulnerable communities and silence certain groups in society. To mitigate such instances, several models have been developed to detect hate and offensive speech. Since detecting hate and offensive speech in social media platforms could incorrectly exclude individuals from social media platforms, which can reduce trust, there is a need to create explainable and interpretable models. Thus, we build an explainable and interpretable high performance model based on the XGBoost algorithm, trained on Twitter data. For unbalanced Twitter data, XGboost outperformed the LSTM, AutoGluon, and ULMFiT models on hate speech detection with an F1 score of 0.75 compared to 0.38 and 0.37, and 0.38 respectively. When we down-sampled the data to three separate classes of approximately 5,000 tweets, XGBoost performed better than LSTM, AutoGluon, and ULMFiT; with F1 scores for hate speech detection of 0.79 vs 0.69, 0.77, and 0.66 respectively. XGBoost also performed better than LSTM, AutoGluon, and ULMFiT in the down-sampled version for offensive speech detection with F1 score of 0.83 vs 0.88, 0.82, and 0.79 respectively. We use Shapley Additive Explanations (SHAP) on our XGBoost models’ outputs to makes it explainable and interpretable compared to LSTM, AutoGluon and ULMFiT that are black-box models. © 2022, Springer Nature Switzerland AG.","2022","2024-12-01 10:11:31","2024-12-14 20:21:30","","233-244","","","13518 LNCS","","","","","","","","Springer Science and Business Media Deutschland GmbH","","English","","","Scopus","","","","Journal Abbreviation: Lect. Notes Comput. Sci.","","","","Transparency; Machine-learning; Social media platforms; Social networking (online); Long short-term memory; Machine learning; Natural language processing systems; Speech recognition; Natural languages; Performance; Language processing; Natural language processing; Xgboost; XGBoost; Speech detection; Hate; Offensive; manual_SHAP; manual_XGBoost; manual_hate","","Chen J.Y.; Fragomeni G.; Degen H.; Ntoa S.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","","","","","","","","","","","","","","",""
"W8JCP38P","journalArticle","2024","Benslimane, S.; Papastergiou, T.; Azé, J.; Bringay, S.; Servajean, M.; Mollevi, C.","A SHAP-based controversy analysis through communities on Twitter","World Wide Web","","1386145X (ISSN)","10.1007/s11280-024-01278-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204002351&doi=10.1007%2fs11280-024-01278-z&partnerID=40&md5=b1f6fbf5d4ee1c74f1efb6befb1171a2","Controversy encompasses content that draws diverse perspectives, along with positive and negative feedback on a specific event, resulting in the formation of distinct user communities. we explore the explainability of controversy through the lens of SHAP (SHapley Additive exPlanations) method, aiming to provide a fair assessment of the individual contributions of different text features of tweets to controversy detection. We conduct an analysis of topic discussions on Twitter from a community perspective, investigating the role of text in accurately classifying tweets into their respective communities. To achieve this, we introduce a SHAP-based pipeline designed to quantify the influence of impactful text features on the predictions of three tweet classifiers. Text content alone offers interesting controversy detection accuracy. It can contain predictive features for controversy detection. For instance, negative connotations, pejorative tendencies and positive qualifying adjectives tend to impact the controversy model detection. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.","2024","2024-12-01 10:11:27","2024-12-14 18:48:34","","","","5","27","","World Wide Web","","","","","","","","English","","","Scopus","","","","Publisher: Springer","","","","Machine-learning; Social networking (online); Machine learning; Explainability; SHAP; Shapley; Shapley additive explanation; User communities; Analysis; Analyze; Communities; Community; Controversy; Text feature; Through the lens; Tweets; manual_BERT; manual_SHAP; manual_controversy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UEBV3XVX","bookSection","2024","Tiwari, R.S.","Hate speech detection using LSTM and explanation by LIME (local interpretable model-agnostic explanations)","Computational Intelligence Methods for Sentiment Analysis in Natural Language Processing Applications","978-044322009-8 (ISBN); 978-044322010-4 (ISBN)","","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191118592&doi=10.1016%2fB978-0-443-22009-8.00005-7&partnerID=40&md5=6d5e0d8a280005ef9fb5af62215a024c","Nowadays, Social Media has a very high impact on everyone’s lives. Social media platforms such as Twitter, Facebook, and Instagram are open platforms where people can express their feelings, thoughts, and emotions. This can lead to a variety of opinions—can collide and lead to verbal conflicts. Therefore, it’s necessary to make social media sites safe by identifying and eliminating hate speech. Any statement that disparages an individual or a group based on a trait such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or another attribute is referred to as hate speeches. Several countries or group of countries such as India, USA, France, Canada, European Unions, and several others have created code of conduct to ensure that social media platforms must regulate any kind of hateful speech, which can disturb the normal functioning of our society and mitigate the risk of any unrest by using social media as a platform. In recent years, we have several examples where fake as well as hate speech on social media has created chaos in real life. Therefore, there is a strict need for an automated methodology to automatically detect and remove hate speech, which can lead to the disturbance in our society. The automated methodology should also be able to provide the clear reason why the specific sentence or the word was detected as hate speech. In this chapter, we will implement a Deep Learning algorithm, that is, Long Short Term Memory to identify hate speech from Twitter speech data, we will be using preprocessing techniques such as Bag of Words, Term Frequency–Inverse Document Frequency, and Glove word embedding by implementing Tensorflow data Pipeline. After training these models, we will compare these models, and based on the metric, we will select all the trained models and implement XAI Explainable Artificial Intelligence known as Local Interpretable Model–Agnostic Explanation to unravel the selected model to understand why the black-box model is associating the datapoint with the specific class. We will discuss theoretical as well as the implementation with the help of python Tensorflow in depth. © 2024 Elsevier Inc. All rights reserved.","2024","2024-12-01 10:11:27","2024-12-14 20:07:44","","93-110","","","","","","","","","","","Elsevier","","English","","","Scopus","","","","Journal Abbreviation: Computational Intelligence Methods for Sentiment Analysis in Natural Language Processing Applications DOI: 10.1016/B978-0-443-22009-8.00005-7","","","","XAI; LIME; Hate speech detection; bag of words; glove; LSTM; TF-IDF; manual_LSTM; manual_LIME; manual_hate","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3Q262JSF","conferencePaper","2023","Yong, W.Y.; Jaiswal, R.; Perez Tellez, F.","Explainability in NLP model: Detection of Covid-19 Twitter Fake News","ACM Int. Conf. Proc. Ser.","979-840071646-1 (ISBN)","","10.1145/3633083.3633212","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183315833&doi=10.1145%2f3633083.3633212&partnerID=40&md5=fc0256110a16d9e5de818347f2d62a85","Fake news has found fertile ground on social media. A global health crisis such as COVID-19 further helps propagate fake news on social media. Much research has been done to develop AI systems that classify news as real or fake. However, there is a growing concern about trust in these AI systems. To this end, we attempt to improve the trustworthiness of AI text classification systems. We use tools to explore data, explain feature extraction techniques, interpret the ML models implemented, and explain the decision-making progress of AI systems. In this study, we compared five ML classifiers for our experiments: Naive Bayes, Support Vector Machines (SVMs), Logistic Regression, Decision Tree, and Random Forest. The models were trained on 10700 tweets containing 5,600 real and 5,100 fake tweets related to COVID-19. In comparison, the SVMs model performance was the best, with a detection accuracy of 0.93 and F1 scores of 0.94 and 0.93 for real and fake news, respectively. Global and local explanations are included to understand the overall model behavior, ensuring transparency and fostering confidence in AI users. We have chosen the SVMs model for the explanation section as it was the best model in this study.  © 2023 ACM.","2023","2024-12-01 10:11:27","2024-12-14 19:28:27","","29-35","","","","","","","","","","","Association for Computing Machinery","","English","","","Scopus","","","","Journal Abbreviation: ACM Int. Conf. Proc. Ser.","","","","Fake news detection; Machine-learning; Social networking (online); Classification (of information); Fake detection; Natural language processing systems; COVID-19; Natural languages; Decision trees; Random forests; Language processing; Natural language processing; Text processing; AI systems; Covid-19 news; Covid-19 News; Explainable AI(XAI); Fake News Detection; Learning classifiers; Logistic regression; Machine learning classifier; Machine Learning Classifier; Natural Language Processing(NLP); Support vector regression; manual_LIME; manual_fake_news; manual_SVM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACM International Conference Proceeding Series","","","","","","","","","","","","","","",""
"Y5Y8CBTK","journalArticle","2024","Mohammadi, H.; Giachanou, A.; Bagheri, A.","A Transparent Pipeline for Identifying Sexism in Social Media: Combining Explainability with Model Prediction","Applied Sciences (Switzerland)","","20763417 (ISSN)","10.3390/app14198620","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206578936&doi=10.3390%2fapp14198620&partnerID=40&md5=fe563f66283c0d4d4eb91450e55cf118","Featured Application: We show illustrative examples of sexist language to describe the taxonomy and explainability analysis. In this study, we present a new approach that combines multiple Bidirectional Encoder Representations from Transformers (BERT) architectures with a Convolutional Neural Network (CNN) framework designed for sexism detection in text at a granular level. Our method relies on the analysis and identification of the most important terms contributing to sexist content using Shapley Additive Explanations (SHAP) values. This approach involves defining a range of Sexism Scores based on both model predictions and explainability, moving beyond binary classification to provide a deeper understanding of the sexism-detection process. Additionally, it enables us to identify specific parts of a sentence and their respective contributions to this range, which can be valuable for decision makers and future research. In conclusion, this study introduces an innovative method for enhancing the clarity of large language models (LLMs), which is particularly relevant in sensitive domains such as sexism detection. The incorporation of explainability into the model represents a significant advancement in this field. The objective of our study is to bridge the gap between advanced technology and human comprehension by providing a framework for creating AI models that are both efficient and transparent. This approach could serve as a pipeline for future studies to incorporate explainability into language models. © 2024 by the authors.","2024","2024-12-01 10:11:27","2024-12-14 18:48:50","","","","19","14","","Appl. Sci.","","","","","","","","English","","","Scopus","","","","Publisher: Multidisciplinary Digital Publishing Institute (MDPI)","","","","Natural language processing systems; Natural languages; Convolutional neural networks; Language processing; Natural language processing; Economic and social effects; Language model; Large language model; ensemble model; Ensemble models; explainable AI (XAI); Explainable AI (XAI); large language models (LLMs); Model prediction; natural language processing (NLP); Prediction models; sexism detection; Sexism detection; Shapley value; Shapley values; manual_SHAP; manual_BERT_w_CNN; manual_sexism","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GYRNXPU3","journalArticle","2024","Siddiqui, J.A.; Yuhaniz, S.S.; Shaikh, G.M.; Soomro, S.A.; Mahar, Z.A.","Fine-Grained Multilingual Hate Speech Detection Using Explainable AI and Transformers","IEEE Access","","21693536 (ISSN)","10.1109/ACCESS.2024.3470901","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205881382&doi=10.1109%2fACCESS.2024.3470901&partnerID=40&md5=c2fb31732085a864b941e9c70fe2f635","The detection of hate speech on online platforms is essential for maintaining safe and inclusive digital environments. Although significant progress has been made in binary classification for hate speech detection, challenges persist in multilingual and fine-grained classification. This study presents a comprehensive model for hate speech detection across English, Urdu, and Sindhi, utilizing advanced deep learning models like Bidirectional Encoder Representations from Transformers (BERT) and its multilingual variants. Additionally, the research employs Explainable Artificial Intelligence (XAI) techniques, such as Local Interpretable Model-Agnostic Explanations (LIME), to gain insights into model performance. This work curated a multilingual hate speech detection dataset and a robust fine-grained hate speech detection model. The dataset includes non-hate and hate speech classes. Furthermore, the hate speech class is categorized into five fine-grained categories, including Disability, Gender, Nationality, Race, and Religion. The experimental findings of this study showed 91% F-score in binary class classification and 86% weighted F-score in fine-grained hate speech detection for multilingual datasets using XLM-RoBERTa technique. Notably, the Religion class achieved the highest F-score of 92%. It is believed that this study contributes to reducing the spread of hate speech (written in Either Urdu, English, or Sindhi) on various social media platforms.  © 2024 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.","2024","2024-12-01 10:11:27","2024-12-14 20:05:13","","143177-143192","","","12","","IEEE Access","","","","","","","","English","","","Scopus","","","","Publisher: Institute of Electrical and Electronics Engineers Inc.","","","","Explainable AI; Deep learning; Language model; hate speech; Hate speech; Speech detection; Fine grained; Large language model; explainable AI; Adversarial machine learning; Contrastive Learning; Digital environment; Distribution transformers; large language model; Low resource languages; low-resource language; Online platforms; manual_BERT; manual_LIME; manual_hate","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SVIG9A57","journalArticle","2024","Hashmi, E.; Yayilgan, S.Y.","Multi-class hate speech detection in the Norwegian language using FAST-RNN and multilingual fine-tuned transformers","Complex and Intelligent Systems","","21994536 (ISSN)","10.1007/s40747-024-01392-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188193309&doi=10.1007%2fs40747-024-01392-5&partnerID=40&md5=7fea2fc47e321857d8538505f569d87e","The growth of social networks has provided a platform for individuals with prejudiced views, allowing them to spread hate speech and target others based on their gender, ethnicity, religion, or sexual orientation. While positive interactions within diverse communities can considerably enhance confidence, it is critical to recognize that negative comments can hurt people’s reputations and well-being. This emergence emphasizes the need for more diligent monitoring and robust policies on these platforms to protect individuals from such discriminatory and harmful behavior. Hate speech is often characterized as an intentional act of aggression directed at a specific group, typically meant to harm or marginalize them based on certain aspects of their identity. Most of the research related to hate speech has been conducted in resource-aware languages like English, Spanish, and French. However, low-resource European languages, such as Irish, Norwegian, Portuguese, Polish, Slovak, and many South Asian, present challenges due to limited linguistic resources, making information extraction labor-intensive. In this study, we present deep neural networks with FastText word embeddings using regularization methods for multi-class hate speech detection in the Norwegian language, along with the implementation of multilingual transformer-based models with hyperparameter tuning and generative configuration. FastText outperformed other deep learning models when stacked with Bidirectional LSTM and GRU, resulting in the FAST-RNN model. In the concluding phase, we compare our results with the state-of-the-art and perform interpretability modeling using Local Interpretable Model-Agnostic Explanations to achieve a more comprehensive understanding of the model’s decision-making mechanisms. © The Author(s) 2024.","2024","2024-12-01 10:11:27","2024-12-15 09:36:37","","4535-4556","","3","10","","Complex Intell. Syst.","","","","","","","","English","","","Scopus","","","","Publisher: Springer International Publishing","","","","Deep Learning; Hate speech; Interpretability modeling; manual_BERT; manual_BERT_w_CNN; manual_FAST-RNN; manual_hate; manual_LIME; Natural language processing; Norwegian language; Transformers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CC8ZV5WN","journalArticle","2024","Dobrojevic, M.; Jovanovic, L.; Babic, L.; Cajic, M.; Zivkovic, T.; Zivkovic, M.; Muthusamy, S.; Antonijevic, M.; Bacanin, N.","Cyberbullying Sexism Harassment Identification by Metaheurustics-Tuned eXtreme Gradient Boosting","Computers, Materials and Continua","","15462218 (ISSN)","10.32604/cmc.2024.054459","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203859892&doi=10.32604%2fcmc.2024.054459&partnerID=40&md5=a24ea497740cc94ce7f73d665206de96","Cyberbullying is a form of harassment or bullying that takes place online or through digital devices like smart-phones, computers, or tablets. It can occur through various channels, such as social media, text messages, online forums, or gaming platforms. Cyberbullying involves using technology to intentionally harm, harass, or intimidate others and may take different forms, including exclusion, doxing, impersonation, harassment, and cyberstalking. Unfortunately, due to the rapid growth of malicious internet users, this social phenomenon is becoming more frequent, and there is a huge need to address this issue. Therefore, the main goal of the research proposed in this manuscript is to tackle this emerging challenge. A dataset of sexist harassment on Twitter, containing tweets about the harassment of people on a sexual basis, for natural language processing (NLP), is used for this purpose. Two algorithms are used to transform the text into a meaningful representation of numbers for machine learning (ML) input: Term frequency inverse document frequency (TF-IDF) and Bidirectional encoder representations from transformers (BERT). The well-known eXtreme gradient boosting (XGBoost) ML model is employed to classify whether certain tweets fall into the category of sexual-based harassment or not. Additionally, with the goal of reaching better performance, several XGBoost models were devised conducting hyperparameter tuning by metaheuristics. For this purpose, the recently emerging Coyote optimization algorithm (COA) was modified and adjusted to optimize the XGBoost model. Additionally, other cutting-edge metaheuristics approach for this challenge were also implemented, and rigid comparative analysis of the captured classification metrics (accuracy, Cohen kappa score, precision, recall, and F1-score) was performed. Finally, the best-generated model was interpreted by Shapley additive explanations (SHAP), and useful insights were gained about the behavioral patterns of people who perform social harassment. Copyright © 2024 The Authors. Published by Tech Science Press.","2024","2024-12-01 10:11:27","2024-12-14 19:02:11","","4997-5027","","3","80","","Comput. Mater. Continua","","","","","","","","English","","","Scopus","","","","Publisher: Tech Science Press","","","","Machine learning; Natural language processing systems; BERT; Bidirectional encoder representation from transformer; Natural languages; Computer crime; Adaptive boosting; Language processing; Natural language processing; NLP; Xgboost; XGBoost; Economic and social effects; Cyber bullying; Inverse problems; Tweets; TF-IDF; Coyote optimization algorithm; Heuristic algorithms; Heuristic methods; Inverse transforms; Metaheuristic; metaheuristics; online harassment and cyberbullying; Online harassment and cyberbullying; Optimization algorithms; Smartphones; Term frequencyinverse document frequency (TF-IDF); Text messaging; manual_SHAP; manual_sexism; manual_XGBoost","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FEHTWPLU","journalArticle","2024","Men, X.; Mariano, V.Y.","Explainable Fake News Detection Based on BERT and SHAP Applied to COVID-19","International Journal of Modern Education and Computer Science","","20750161 (ISSN)","10.5815/ijmecs.2024.01.02","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184467170&doi=10.5815%2fijmecs.2024.01.02&partnerID=40&md5=968c742aeedfa446d4c4f029c5a05aba","Fake news detection has become a significant research top in natural language processing. Since the outbreak of the covid-19 epidemic, a large amount of fake news about covid-19 has spread on social media, making the detection of fake news a challenging task. Applying deep learning models may improve predictions. However, their lack of explainability poses a challenge to their widespread adoption and use in practical applications. This work aims to design a deep learning framework for accurate and explainable prediction of covid-19 fake news. First, we choose BiLSTM as the base model and improve the classification performance of the BiLSTM model by incorporating BERT-based distillation. Then, a post-hoc interpretation method SHAP is used to explain the classification results of the model to improve the transparency of the model and increase people's confidence in the practical application. Finally, utilizing visual interpretation methods, such as significance plots, to analyze specific sample classification results for gaining insights into the key terms that influence the model’s decisions. Ablation experiments demonstrated the reliability of the explainable method. © 2024, Modern Education and Computer Science Press. All rights reserved.","2024","2024-12-01 10:11:27","2024-12-15 09:35:18","","11-22","","1","16","","Int. J. Mod. Educ. Comput. Sci.","","","","","","","","English","","","Scopus","","","","Publisher: Modern Education and Computer Science Press","","","","BERT; Covid-19; Explainability; Fake news; Knowledge Distillation; manual_BERT; manual_fake_news; manual_SHAP; SHAP","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XJZZ8VW5","journalArticle","2024","Sharma, S.; Sharma, R.; Datta, A.","(Mis)leading the COVID-19 Vaccination Discourse on Twitter: An Exploratory Study of Infodemic Around the Pandemic","IEEE Transactions on Computational Social Systems","","2329924X (ISSN)","10.1109/TCSS.2022.3225216","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144755761&doi=10.1109%2fTCSS.2022.3225216&partnerID=40&md5=3398ab1f994e8c81b94ce31cf5adc9e9","In this work, we collect a moderate-sized representative corpus of tweets (over 200 000) pertaining to COVID-19 vaccination spanning for a period of seven months (September 2020-March 2021). Following a transfer learning approach, we utilize a pretrained transformer-based XLNet model to classify tweets as misleading or nonmisleading and manually validate the results with random subsets of samples. We leverage this to study and contrast the characteristics of tweets in the corpus that are misleading in nature against non-misleading ones. This exploratory analysis enables us to design features such as sentiments, hashtags, nouns, and pronouns which can, in turn, be exploited for classifying tweets as (non-)misleading using various machine learning (ML) models in an explainable manner. Specifically, several ML models are employed for prediction, with up to 90% accuracy, with the importance of each feature is explained using SHAP Explainable AI (XAI) tool. While the thrust of this work is principally exploratory in nature to obtain insight on the online discourse on COVID-19 vaccination, we conclude the article by outlining how these insights provide the foundations for a more actionable approach to mitigate misinformation. We have made the curated data as well as the accompanying code available so that the research community at large can reproduce, compare against, or build upon this work. © 2014 IEEE.","2024","2024-12-01 10:11:27","2024-12-14 20:30:21","","352-362","","1","11","","IEEE Trans. Computat. Soc. Syst.","","","","","","","","English","","","Scopus","","","","Publisher: Institute of Electrical and Electronics Engineers Inc.","","","","Fake news; Social media; Learning systems; Machine learning models; Social networking (online); Fake detection; Blogs; COVID-19; social media; Transformer; Vaccines; misinformation; Misinformation; explainable AI (XAI); Explainable AI (XAI); Exploratory studies; Learning approach; Transfer learning; manual_SHAP; manual_misinformation; manual_random_forest","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YFJPX7MW","journalArticle","2024","Hashmi, E.; Yildirim Yayilgan, S.; Hameed, I.A.; Mudassar Yamin, M.; Ullah, M.; Abomhara, M.","Enhancing Multilingual Hate Speech Detection: From Language-Specific Insights to Cross-Linguistic Integration","IEEE Access","","21693536 (ISSN)","10.1109/ACCESS.2024.3452987","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203525883&doi=10.1109%2fACCESS.2024.3452987&partnerID=40&md5=c91420a214124b1abf97ec77308af7ef","The rise of social media has enabled individuals with biased perspectives to spread hate speech, directing it toward individuals based on characteristics such as race, gender, religion, or sexual orientation. Constructive interactions in varied communities can greatly enhance self-esteem, yet it is vital to consider that adverse comments may affect individuals' social standing and emotional health. The crucial task of detecting and addressing this type of content is imperative for reducing its negative effects on communities and individuals alike. The rising occurrence highlights the urgency for enhanced methods and robust regulations on digital platforms to protect humans from such prejudicial and damaging conduct. Hate speech typically appears as a deliberate hostile action aimed at a particular group, often with the intent to demean or isolate them based on various facets of their identity. Research on hate speech predominantly targets resource-aware languages like English, German, and Chinese. Conversely, resource-limited languages, including European languages such as Italian, Spanish, and Portuguese, alongside Asian languages like Roman Urdu, Korean, and Indonesian, present obstacles. These challenges arise from a lack of linguistic resources, making the extraction of information a more strenuous task. This study is focused on the detection and improvement of multilingual hate speech detection across 13 different languages. To conduct a thorough analysis, we carried out a series of experiments that ranged from classical machine learning techniques and mainstream deep learning approaches to recent transformer-based methods. Through hyperparameter tuning, optimization techniques, and generative configurations, we achieved robust and generalized performance capable of effectively identifying hate speech across various dialects. Specifically, we achieved a notable enhancement in detection performance, with precision and recall metrics exceeding baseline models by up to 10% across several lesser-studied languages. Additionally, our work extends the capabilities of explainable AI within this context, offering deeper insights into model decisions, which is crucial for regulatory and ethical considerations in AI deployment. Our study presents substantial performance improvements across various datasets and languages through meticulous comparisons. For example, our model significantly outperformed existing benchmarks: it achieved F1-scores of 0.90 in German (GermEval-2018), up from the baseline score of 0.72, and 0.93 in German (GermEval-2021), a substantial increase from 0.58. Additionally, it scored 0.95 in Roman Urdu HS, surpassing the previous peak of 0.91. Furthermore, for mixed-language datasets such as Italian and English (AMI 2018), our accuracy rose dramatically from 0.59 to 0.96. These outcomes emphasize the robustness and versatility of our model, establishing a new standard for hate speech detection systems across diverse linguistic settings. © 2024 The Authors.","2024","2024-12-01 10:11:27","2024-12-14 19:20:54","","121507-121537","","","12","","IEEE Access","","","","","","","","English","","","Scopus","","","","Publisher: Institute of Electrical and Electronics Engineers Inc.","","","","Explainable AI; Embeddings; Machine-learning; deep learning; Deep learning; Natural language processing systems; Natural languages; machine learning; Language processing; Natural language processing; Economic and social effects; Benchmarking; Transformer; Hate speech; natural language processing; word embedding; Word embedding; explainable AI; Adversarial machine learning; Speech enhancement; Contrastive Learning; Energy security; transformers; manual_LIME; manual_hate; manual_mBERT","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEJW28T7","journalArticle","2024","Ansari, G.; Kaur, P.; Saxena, C.","Data Augmentation for Improving Explainability of Hate Speech Detection","Arabian Journal for Science and Engineering","","2193567X (ISSN)","10.1007/s13369-023-08100-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165014094&doi=10.1007%2fs13369-023-08100-4&partnerID=40&md5=fc57e8f3374ea5df5ef50d7808828204","The paper presents a novel data augmentation-based approach to develop explainable, deep learning models for hate speech detection. Hate speech is widely prevalent on online social media but difficult to detect automatically due to challenges of natural language processing and complexity of hate speech. Further, the decisions of the existing solutions possess constrained explainability since limited annotated data are available for training and testing of models. Therefore, this work proposes the use of text-based data augmentation for improving the performance and explainability of deep learning models. Techniques based on easy data augmentation, bidirectional encoder representations from transformers and back translation have been utilized for data augmentation. Convolutional neural networks and long short-term memory models are trained with augmented data and evaluated on two publicly available datasets for hate speech detection. Methods of LIME and integrated gradients are used to retrieve explanations of the deep learning models. A diagnostic study is conducted on test samples to check for improvement in the models as a result of the data augmentation. The experimental results verify that the proposed approach improves the explainability as well as the accuracy of hate speech detection. © King Fahd University of Petroleum & Minerals 2023.","2024","2024-12-01 10:11:26","2024-12-14 19:05:11","","3609-3621","","3","49","","Arab. J. Sci. Eng.","","","","","","","","English","","","Scopus","","","","Publisher: Institute for Ionics","","","","Explainable AI; LIME; Hate speech; Cyberbullying; Data augmentation; Integrated gradient; manual_LSTM; manual_LIME_w_integrated_gradients; manual_CNN; manual_hate","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AJP585CX","journalArticle","2024","Imbwaga, J.L.; Chittaragi, N.B.; Koolagudi, S.G.","Explainable hate speech detection using LIME","International Journal of Speech Technology","","13812416 (ISSN)","10.1007/s10772-024-10135-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202689010&doi=10.1007%2fs10772-024-10135-3&partnerID=40&md5=c50ee3cad8f39a9668a5c3fd6595984c","Free speech is essential, but it can conflict with protecting marginalized groups from harm caused by hate speech. Social media platforms have become breeding grounds for this harmful content. While studies exist to detect hate speech, there are significant research gaps. First, most studies used text data instead of other modalities such as videos or audio. Second, most studies explored traditional machine learning algorithms. However, due to the increase in complexities of computational tasks, there is need to employ complex techniques and methodologies. Third, majority of the research studies have either been evaluated using very few evaluation metrics or not statistically evaluated at all. Lastly, due to the opaque, black-box nature of the complex classifiers, there is need to use explainability techniques. This research aims to address these gaps by detecting hate speech in English and Kiswahili languages using videos manually collected from YouTube. The videos were converted to text and used to train various classifiers. The performance of these classifiers was evaluated using various evaluation and statistical measurements. The experimental results suggest that the random forest classifier achieved the highest results for both languages across all evaluation measurements compared to all classifiers used. The results for English language were: accuracy 98%, AUC 96%, precision 99%, recall 97%, F1 98%, specificity 98% and MCC 96% while the results for Kiswahili language were: accuracy 90%, AUC 94%, precision 93%, recall 92%, F1 94%, specificity 87% and MCC 75%. These results suggest that the random forest classifier is robust, effective and efficient in detecting hate speech in any language. This also implies that the classifier is reliable in detecting hate speech and other related problems in social media. However, to understand the classifiers’ decision-making process, we used the Local Interpretable Model-agnostic Explanations (LIME) technique to explain the predictions achieved by the random forest classifier. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.","2024","2024-12-01 10:11:26","2024-12-14 19:50:18","","793-815","","3","27","","Int J Speech Technol","","","","","","","","English","","","Scopus","","","","Publisher: Springer","","","","Machine learning; Speech recognition; BERT; Decision trees; Random forests; Economic and social effects; Hate speech; Speech detection; Explainable AI (XAI); Local interpretable model-agnostic explanation; Local Interpretable Model-agnostic Explanations (LIME); Free speech; GPT-J-6b; GPT-J-6B; Kiswahili; Kiswahilus; Random forest classifier; Whisper AI; manual_BERT; manual_LIME; manual_hate","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8EFC3X2D","journalArticle","2024","Mazhar, K.; Dwivedi, P.","Decoding the black box: LIME-assisted understanding of Convolutional Neural Network (CNN) in classification of social media tweets","Social Network Analysis and Mining","","18695450 (ISSN)","10.1007/s13278-024-01297-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198029775&doi=10.1007%2fs13278-024-01297-8&partnerID=40&md5=a5e3f0f8731490ec6505f9a0c5406455","The rise of social media has brought both opportunities and challenges to the digital age, including the proliferation of online trolls that have spread misinformation, hates, and disruptions. An automated classification system is crucial to mitigate the impact of trolls. This paper presents an innovative approach for classifying social media tweets into troll and non-troll categories using a machine learning (ML) approach and CNN. We also employed explainable artificial intelligence (XAI) to address the inherent opacity and complexity of the CNN model. This approach allowed us to provide a comprehensive explanation of the model’s behavior. We have achieved Accuracy = 91.45% with CNN2 model. The best results using ML methods were achieved by random forest classifier model, Accuracy = 86.57%. To enhance our trust in the CNN model, we leveraged the local interpretable model-agnostic explanation (LIME) technique within XAI. Algorithm correctly predicted troll tweets with a confidence of 93% and non-troll tweets with a confidence of 97%. This research lays the groundwork for better decision making in the ever-changing field of social media content analysis by bridging the gap between complex neural networks and insights that can be understood by humans. The transparency and reliability that LIME brings to public discussion are crucial tools for ensuring the responsible and efficient use of online content, as social media continues to influence public opinion. © The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature 2024.","2024","2024-12-01 10:11:26","2024-12-14 19:06:18","","","","1","14","","Soc. Netw. Analysis Min.","","","","","","","","English","","","Scopus","","","","Publisher: Springer","","","","Social media; Machine-learning; Complex networks; Social networking (online); Machine learning; Lime; Black boxes; Decision making; Neural network models; Convolutional neural network; Convolutional neural networks; Neural network model; Convolution; Social aspects; Local interpretable model-agnostic explanation; Automated classification systems; Convolutional Neural Networks (CNNs); Digital age; Forestry; Local interpretable model-agnostic explanations (LIME); Twitter tweet; Twitter tweets; manual_LIME; manual_CNN; manual_troll","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P73J4HF5","journalArticle","2024","Kibriya, H.; Siddiqa, A.; Khan, W.Z.; Khan, M.K.","Towards safer online communities: Deep learning and explainable AI for hate speech detection and classification","Computers and Electrical Engineering","","00457906 (ISSN)","10.1016/j.compeleceng.2024.109153","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186659253&doi=10.1016%2fj.compeleceng.2024.109153&partnerID=40&md5=97887a976a292f41a50b053f82ec1d40","The internet and social media facilitate widespread idea sharing but also contribute to cyber-crimes and harmful behaviors, notably the dissemination of abusive and hateful speech, which poses a significant threat to societal cohesion. Hence, prompt and accurate detection of such harmful content is crucial. To address this issue, our study introduces a fully automated end-to-end model for hate speech detection and classification using Natural Language Processing and Deep Learning techniques. The proposed architecture comprising embedding, Convolutional, bidirectional Recurrent Neural Network, and bidirectional Long Short Term Memory layers, achieved the highest accuracy of 98.5%. Additionally, we employ explainable AI techniques, such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), to gain insights into the performance of the proposed framework. This comprehensive approach meets the pressing demand for swift and precise detection and categorization of harmful online content. © 2024 Elsevier Ltd","2024","2024-12-01 10:11:26","2024-12-14 21:18:50","","","","","116","","Comput Electr Eng","","","","","","","","English","","","Scopus","","","","Publisher: Elsevier Ltd","","","","Social media; Learning systems; Machine-learning; Deep learning; On-line communities; Social networking (online); Machine learning; Explainable artificial intelligence; Lime; Multilayer neural networks; Natural language processing systems; Speech recognition; Explainable Artificial Intelligence; Recurrent neural networks; Speech classification; Hate speech; Speech detection; E-learning; Hate speech detection; Toxic comment; Toxic comments; manual_SHAP; manual_LSTM; manual_LIME; manual_hate","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W8MTP7A9","journalArticle","2025","Hashmi, E.; Yayilgan, S.Y.; Yamin, M.M.; Ullah, M.","Enhancing misogyny detection in bilingual texts using explainable AI and multilingual fine-tuned transformers","Complex and Intelligent Systems","","21994536 (ISSN)","10.1007/s40747-024-01655-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209140744&doi=10.1007%2fs40747-024-01655-1&partnerID=40&md5=da6b8a926aa53a4382f03e16f4732e6a","Gendered disinformation undermines women’s rights, democratic principles, and national security by worsening societal divisions through authoritarian regimes’ intentional weaponization of social media. Online misogyny represents a harmful societal issue, threatening to transform digital platforms into environments that are hostile and inhospitable to women. Despite the severity of this issue, efforts to persuade digital platforms to strengthen their protections against gendered disinformation are frequently ignored, highlighting the difficult task of countering online misogyny in the face of commercial interests. This growing concern underscores the need for effective measures to create safer online spaces, where respect and equality prevail, ensuring that women can participate fully and freely without the fear of harassment or discrimination. This study addresses the challenge of detecting misogynous content in bilingual (English and Italian) online communications. Utilizing FastText word embeddings and explainable artificial intelligence techniques, we introduce a model that enhances both the interpretability and accuracy in detecting misogynistic language. To conduct an in-depth analysis, we implemented a range of experiments encompassing classic machine learning methodologies and conventional deep learning approaches to the recent transformer-based models incorporating both language-specific and multilingual capabilities. This paper enhances the methodologies for detecting misogyny by incorporating incremental learning for cutting-edge datasets containing tweets and posts from different sources like Facebook, Twitter, and Reddit, with our proposed approach outperforming these datasets in metrics such as accuracy, F1-score, precision, and recall. This process involved refining hyperparameters, employing optimization techniques, and utilizing generative configurations. By implementing Local Interpretable Model-agnostic Explanations (LIME), we further elucidate the rationale behind the model’s predictions, enhancing understanding of its decision-making process. © The Author(s) 2024.","2025","2024-12-01 10:11:26","2024-12-14 19:19:00","","","","1","11","","Complex Intell. Syst.","","","","","","","","English","","","Scopus","","","","Publisher: Springer International Publishing","","","","Explainable AI; Deep Learning; Machine Learning; Transformers; FastText; Misogyny; manual_LIME; manual_ELECTRA; manual_misogyny","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"42FCKPDY","conferencePaper","2023","Yadav, S.; Kaushik, A.; McDaid, K.","Hate Speech is not Free Speech: Explainable Machine Learning for Hate Speech Detection in Code-Mixed Languages","Int Symp Technol Soc","979-835032486-0 (ISBN)","","10.1109/ISTAS57930.2023.10305996","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178521180&doi=10.1109%2fISTAS57930.2023.10305996&partnerID=40&md5=b7709df970cc3c5e5027a085a0b91fb3","The increase in connectivity provided by social media platforms comes with several disadvantages. It has become surprisingly easy for ill-intentioned individuals to stalk, harass and threaten others online on the basis of race, gender, religion, etc. Artificial intelligence models provide a valuable solution to the problem by automatically filtering such content. However, research for hate speech detection in low-resource languages is still nascent. To promote research in the area, several shared tasks are held in various languages and attract contributions with novel approaches. In this study, we use the English and Hindi code-mixed datasets provided by the HASOC Identification of Conversational Hate-Speech in Code-Mixed Languages (ICHCL) shared task to train and evaluate machine learning models and compare the performance of different vectorization techniques and model parameters. The Explainable Artificial Intelligence (XAI) technique Local Interpretable Model Agnostic Explanation (LIME) is also applied to the model results to ascertain if the model is behaving in a coherent and logical manner. The highest accuracy achieved is 67.61 % using Bernoulli Naive Bayes with count vectorizer and TF-IDF. The analysis suggests that many of the models may be heavily influenced by the presence of slur words while classifying a statement as hateful and offensive.  © 2023 IEEE.","2023","2024-12-01 10:11:29","2024-12-14 20:12:00","","","","","","","","","","","","","Institute of Electrical and Electronics Engineers Inc.","","English","","","Scopus","","","","Journal Abbreviation: Int Symp Technol Soc","","","","Machine-learning; Machine learning; Explainable artificial intelligence; Lime; LIME; Sentiment analysis; Speech recognition; machine learning; Internet of things; hate speech; Hate speech; Speech detection; explainable artificial intelligence; sentiment analysis; Free speech; Hinglish; Local interpretable model agnostic explanation; mix-code; Mix-code; word2vec; Word2vec; manual_LIME; manual_hate; manual_Naive_Bayes","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Symposium on Technology and Society, Proceedings","","","","","","","","","","","","","","",""
"HTDVXI9X","journalArticle","2024","Pospelova, N.; Tarasova, A.; Subbotina, N.; Koroleva, N.; Raimova, N.; Lydia, E.L.","Explainable Artificial Intelligence and Natural Language Processing for Unraveling Deceptive Contents","Fusion: Practice and Applications","","27700070 (ISSN)","10.54216/FPA.140212","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186230970&doi=10.54216%2fFPA.140212&partnerID=40&md5=8b01c7692f80f693419d4f236ad0ea88","Deceptive content recognition in social media employing artificial intelligence (AI) includes the use of sophisticated techniques and machine learning (ML) methods to recognize deceptive or wrong data shared on numerous platforms. AI methods analyse textual as well as multimedia content, investigative patterns, linguistic cues, and contextual info to flag latent cases of deception. As a result of the use of natural language processing (NLP) and computer vision (CV), these systems identify subtle nuances, misrepresentation strategies, and anomalies in user-generated content. This active technique permits social media platforms, organizations, and consumers to recognize and diminish the spread of deceptive content, donates to a more reliable online atmosphere, and aids in fighting tasks modelled by misinformation and false news. This study offers a novel sine cosine algorithm with deep learning-based deceptive content detection on social media (SCADL-DCDSM) technique. The SCADL-DCDSM technique incorporates the ensemble learning process with a hyperparameter tuning strategy for classifying the sentiments. Primarily, the SCADL-DCDSM technique pre-processes the input data to change the input data into a valuable format. Moreover, the SCADL-DCDSM algorithm follows the BERT model for the word embedding process. Moreover, the SCADL-DCDSM technique involves an ensemble of three models for sentiment classification such as long short-term memory (LSTM), extreme learning machine (ELM), and attention-based recurrent neural network (ARNN). Finally, SCA can be executed for better hyperparameter choice of the DL models. The SCADL-DCDSM system integrates the explainable artificial intelligence (XAI) system LIME has been employed for a comprehensive explainability and understanding of the black-box process, enhancing correct deceptive content recognition. The simulation result analysis of the SCADL-DCDSM algorithm has been examined on a benchmark database. The simulation outcome illustrated that the SCADL-DCDSM methodology achieves optimum solution than other approaches in terms of different measures. © 2024, American Scientific Publishing Group (ASPG). All rights reserved.","2024","2024-12-01 10:11:29","2024-12-14 19:41:09","","146-158","","2","14","","Fusion. Pract. Appl.","","","","","","","","English","","","Scopus","","","","Publisher: American Scientific Publishing Group (ASPG)","","","","Natural Language Processing; BERT; Explainable Artificial Intelligence; Social Media; Word Embedding; manual_BERT; manual_LSTM; manual_LIME; manual_fake_news","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9DCQRIP9","conferencePaper","2023","Shevtsov, A.; Antonakaki, D.; Lamprou, I.; Kontogiorgakis, I.; Pratikakis, P.; Ioannidis, S.","Russo-Ukrainian War: Prediction and explanation of Twitter suspension","Proc. IEEE/ACM Int. Conf. Adv. Soc. Networks Anal. Mining, ASONAM","979-840070409-3 (ISBN)","","10.1145/3625007.3627317","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190628121&doi=10.1145%2f3625007.3627317&partnerID=40&md5=b5cb8ee894cb0ed777e4ea9475614933","On 24 February 2022, Russia invaded Ukraine, starting what is now known as the Russo-Ukrainian War, initiating an online discourse on SNs. Twitter one of the most popular SNs, with an open and democratic character, enables a transparent discussion among its large user base. Unfortunately, this often leads to Twitter's policy violations, propaganda, abusive actions, civil integrity violations, and consequently to user accounts' suspension and deletion. This study focuses on the Twitter suspension mechanism and the analysis of shared content and features leading to an accurate machine-learning suspension prediction. Toward this goal, we have obtained a dataset containing 107.7M tweets, originating from 9.8 million users, using Twitter API. We extract the categories of shared content of the suspended accounts and explain their characteristics, through the extraction of text embeddings in junction with cosine similarity clustering. Our results reveal scam campaigns taking advantage of trending topics regarding the Russia-Ukrainian conflict for Bitcoin and Ethereum fraud, spam, and advertisement campaigns. Additionally, we apply a ML methodology including a SHapley Additive explainability model to understand and explain how user accounts get suspended. © 2023 Owner/Author(s).","2023","2024-12-01 10:11:29","2024-12-14 20:58:10","","348-355","","","","","","","","","","","Association for Computing Machinery, Inc","","English","","","Scopus","","","","Journal Abbreviation: Proc. IEEE/ACM Int. Conf. Adv. Soc. Networks Anal. Mining, ASONAM","","","","ML; Social networking (online); Explainability; SHAP; explainability; Twitter; Large users; russo-ukrainian war; Russo-ukrainian war; Shared contents; Suspension mechanism; Ukraine; user suspension; User suspension; manual_SHAP; manual_GNN; manual_suspension","","Aditya Prakash B.; Wang D.; Weninger T.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2023","","","","","","","","","","","","","","",""
"LU78JQPB","journalArticle","2023","Dua, V.; Rajpal, A.; Rajpal, S.; Agarwal, M.; Kumar, N.","I-FLASH: Interpretable Fake News Detector Using LIME and SHAP","Wireless Personal Communications","","09296212 (ISSN)","10.1007/s11277-023-10582-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164528990&doi=10.1007%2fs11277-023-10582-2&partnerID=40&md5=01e984cb005cc5b1385b53c73209f6aa","The rise of social media enables people to disseminate information. However, when false but appealing information is presented as news, it becomes a cause for serious concern as it might lead to a harmful influence on communities of innocent believers. To address this issue, we propose I-FLASH, an interpretable fake news detector that not only detects fake news but also explains why it considers some content fake or genuine. Moreover, recent research evaluated their models for fake news detection on domain-specific datasets. Therefore, in this paper, two new tiny datasets, FactCheck and FactCheck2, were culled from the official Twitter accounts/websites of various well-known media outlets, covering a variety of other societal domains such as education, crime, and technology. We also compared the performance of the machine learning model (logistic regression with TF-IDF), deep learning model (bidirectional LSTM with GloVe word embeddings), and the pre-trained Bidirectional Encoder Representations from Transformers (BERT) model on curated datasets along with two other popular datasets, namely, LIAR and COVID-19. The stratified 10-fold cross-validation accuracy of 94.41 ± 0.38% on the COVID-19 dataset, 61.18 ± 0.55% on the LIAR dataset, 87.25 ± 2.45% on FactCheck, and 92.91 ± 2.07% on FactCheck2, attained at 95% confidence interval, establishes the efficacy of models. On cross-dataset validation, we observe that the model trained on a generalized dataset like FactCheck2 can perform well on domain-specific datasets like COVID-19 and LIAR with a validation accuracy of 64.25% and 54.22%, respectively. Finally, using XAI methods—LIME and SHAP revealed important terms while predicting the news class (fake/real). © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","2023","2024-12-01 10:11:28","2024-12-14 20:13:51","","2841-2874","","4","131","","Wireless Pers Commun","","","","","","","","English","","","Scopus","","","","Publisher: Springer","","","","Explainable AI; Fake news detection; Social media; Learning systems; Machine learning models; Social networking (online); Long short-term memory; Lime; LIME; Fake detection; Information dissemination; Recent researches; BERT; Bidirectional encoder representation from transformer; COVID-19; Performance; SHAP; Media outlets; Domain specific; manual_BERT; manual_SHAP; manual_LIME; manual_fake_news","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SWYJUUWR","conferencePaper","2024","Islam, M.R.; Bataineh, A.S.; Zulkernine, M.","Detection of Cyberbullying in Social Media Texts Using Explainable Artificial Intelligence","Commun. Comput. Info. Sci.","18650929 (ISSN); 978-981971273-1 (ISBN)","","10.1007/978-981-97-1274-8_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189620841&doi=10.1007%2f978-981-97-1274-8_21&partnerID=40&md5=240c4ba2f4a0716139819e5dd2738fe4","The widespread use of social media has opened the door to new forms of harassment and abuse, such as cyberbullying, that have a serious impact on individuals’ psychological health. Therefore, research communities have recently developed detection approaches using Natural Language Processing (NLP) combined with machine learning algorithms to identify instances of cyberbullying in social media texts. However, they are unable to determine the type of cyberbullying and the reasons why victims may be targeted. This paper develops a novel detection approach that can identify the type of cyberbullying based on characteristics such as gender, religion, age, and ethnicity, even if the original records in the training dataset do not include such information or features. This paper has accomplished this objective by utilizing Explainable Artificial Intelligence (XAI) technology alongside machine learning models to justify and explain the classification of text as cyberbullying. Technically speaking, XAI technology enables machine learning models to capture and highlight the most influential words that affect the decision to classify a text as cyberbullying. Those influential words are utilized to re-label and update the training data. The machine learning models are then re-trained using the updated data. To evaluate the performance of the proposed approach, a simulation experiment has been conducted on a large dataset containing texts from Twitter. Simulation results show that XAI technology provides convincing explanations for classifying a text as cyberbullying. It also enables machine learning models to identify various types of cyberbullying and enhances their performance in terms of classification accuracy. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.","2024","2024-12-01 10:11:28","2024-12-14 19:14:56","","319-334","","","2034 CCIS","","","","","","","","Springer Science and Business Media Deutschland GmbH","","English","","","Scopus","","","","Journal Abbreviation: Commun. Comput. Info. Sci.","","","","Social media; Machine learning models; Social networking (online); Learning algorithms; Machine learning; Lime; Natural language processing systems; Natural languages; Performance; Computer crime; Explainable artificial intelligence (XAI); Language processing; Natural language processing; Text processing; Cyber bullying; Cyberbullying; Local interpretable model-agnostic explanation; Natural Language Processing (NLP); Explainable Artificial Intelligence (XAI); Detection approach; Large datasets; Local Interpretable Model-Agnostic Explanations (LIME); manual_LSTM; manual_LIME; manual_cyberbullying; manual_random_forest; manual_gradient_boosting; manual_SVM","","Wang G.; Wang H.; Min G.; Georgalas N.; Meng W.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Communications in Computer and Information Science","","","","","","","","","","","","","","",""
"YX4K498U","conferencePaper","2023","Zhang, K.; Yu, J.; Shi, H.; Liang, J.; Zhang, X.-Y.","Rumor Detection with Diverse Counterfactual Evidence","Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.","979-840070103-0 (ISBN)","","10.1145/3580305.3599494","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171363868&doi=10.1145%2f3580305.3599494&partnerID=40&md5=87de4c3f36c745bf207f065acc79d90d","The growth in social media has exacerbated the threat of fake news to individuals and communities. This draws increasing attention to developing efficient and timely rumor detection methods. The prevailing approaches resort to graph neural networks (GNNs) to exploit the post-propagation patterns of the rumor-spreading process. However, these methods lack inherent interpretation of rumor detection due to the black-box nature of GNNs. Moreover, these methods suffer from less robust results as they employ all the propagation patterns for rumor detection. In this paper, we address the above issues with the proposed Diverse Counterfactual Evidence framework for Rumor Detection (DCE-RD). Our intuition is to exploit the diverse counterfactual evidence of an event graph to serve as multi-view interpretations, which are further aggregated for robust rumor detection results. Specifically, our method first designs a subgraph generation strategy to efficiently generate different subgraphs of the event graph. We constrain the removal of these subgraphs to cause the change in rumor detection results. Thus, these subgraphs naturally serve as counterfactual evidence for rumor detection. To achieve multi-view interpretation, we design a diversity loss inspired by Determinantal Point Processes (DPP) to encourage diversity among the counterfactual evidence. A GNN-based rumor detection model further aggregates the diverse counterfactual evidence discovered by the proposed DCE-RD to achieve interpretable and robust rumor detection results. Extensive experiments on two real-world datasets show the superior performance of our method. Our code is available at https://github.com/Vicinity111/DCE-RD.  © 2023 Owner/Author.","2023","2024-12-01 10:11:28","2024-12-14 21:09:35","","3321-3331","","","","","","","","","","","Association for Computing Machinery","","English","","","Scopus","","","","Journal Abbreviation: Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.","","","","Propagation pattern; graph neural network; Fake detection; Backpropagation; Graph neural networks; Counterfactuals; Rumor detection; counterfactual evidence; Counterfactual evidence; Event graphs; Evidence framework; interpretation; Interpretation; Multi-views; rumor detection; Subgraphs; manual_counterfactual; manual_GNN; manual_rumours","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","","","","","","","","","","","","","","",""
"KYEJ7ABQ","conferencePaper","2023","Upadhyay, R.; Pasi, G.; Viviani, M.","Leveraging Socio-contextual Information in BERT for Fake Health News Detection in Social Media","Proc. Workshop Open Challenges Online Soc. Networks, OASIS , Held conjunction ACM Conf. Hypertext Soc. Media, HT","979-840070225-9 (ISBN)","","10.1145/3599696.3612902","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174179013&doi=10.1145%2f3599696.3612902&partnerID=40&md5=02370b6d567ce5da2dc35bcf95ddfa7a","Fake news is a major challenge in social media, particularly in the health domain where it can lead to severe consequences for both individuals and society as a whole. To contribute to combating this problem, we present a novel solution for improving the accuracy of detecting fake health news, utilizing a fine-Tuned BERT model that integrates both user-and content-related socio-contextual information. Specifically, this information is combined with the textual content itself to form a socio-contextual input sequence for the BERT model. By fine-Tuning such a model with respect to the health misinformation detection task, the resulting classifier can accurately predict the category to which each piece of content belongs, i.e., either ""real health news""or ""fake health news"". We validate our solution through a series of experiments conducted on distinct publicly available datasets constituted by health-related tweets. These results illustrate the superiority of the proposed solution compared to the standard BERT baseline model and other advanced models. Indeed, they show that the integration of socio-contextual information in the detection process positively contributes to increasing the overall accuracy of the fake health news detection task. The study also suggests, in a preliminary way, how such information could be used for the explainability of the model itself. © 2023 Owner/Author.","2023","2024-12-01 10:11:28","2024-12-14 20:24:24","","38-46","","","","","","","","","","","Association for Computing Machinery, Inc","","English","","","Scopus","","","","Journal Abbreviation: Proc. Workshop Open Challenges Online Soc. Networks, OASIS , Held conjunction ACM Conf. Hypertext Soc. Media, HT","","","","Fake news; Social media; Social networking (online); Classification (of information); Fake detection; Fake News; BERT; Social Media; Classification; Transformer; Transformers; Detection tasks; Contextual information; Health misinformation; Health Misinformation; Novel solutions; Social context; Social Context; Textual content; manual_BERT; manual_SHAP; manual_fake_news","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the 2023 Workshop on Open Challenges in Online Social Networks, OASIS 2023, Held in conjunction with the 34th ACM conference on Hypertext and Social Media, HT 2023","","","","","","","","","","","","","","",""
"GDEIRIHY","journalArticle","2024","Hashmi, E.; Yayilgan, S.Y.; Yamin, M.M.; Ali, S.; Abomhara, M.","Advancing Fake News Detection: Hybrid Deep Learning With FastText and Explainable AI","IEEE Access","","21693536 (ISSN)","10.1109/ACCESS.2024.3381038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189176685&doi=10.1109%2fACCESS.2024.3381038&partnerID=40&md5=c9b4eaf0cab966cccdeff8a99d477b12","The widespread propagation of misinformation on social media platforms poses a significant concern, prompting substantial endeavors within the research community to develop robust detection solutions. Individuals often place unwavering trust in social networks, often without discerning the origins and authenticity of the information disseminated through these platforms. Hence, the identification of media-rich fake news necessitates an approach that adeptly leverages multimedia elements and effectively enhances detection accuracy. The ever-changing nature of cyberspace highlights the need for measures that may effectively resist the spread of media-rich fake news while protecting the integrity of information systems. This study introduces a robust approach for fake news detection, utilizing three publicly available datasets: WELFake, FakeNewsNet, and FakeNewsPrediction. We integrated FastText word embeddings with various Machine Learning and Deep Learning methods, further refining these algorithms with regularization and hyperparameter optimization to mitigate overfitting and promote model generalization. Notably, a hybrid model combining Convolutional Neural Networks and Long Short-Term Memory, enriched with FastText embeddings, surpassed other techniques in classification performance across all datasets, registering accuracy and F1-scores of 0.99, 0.97, and 0.99, respectively. Additionally, we utilized state-of-the-art transformer-based models such as BERT, XLNet, and RoBERTa, enhancing them through hyperparameter adjustments. These transformer models, surpassing traditional RNN-based frameworks, excel in managing syntactic nuances, thus aiding in semantic interpretation. In the concluding phase, explainable AI modeling was employed using Local Interpretable Model-Agnostic Explanations, and Latent Dirichlet Allocation to gain deeper insights into the model's decision-making process. © 2013 IEEE.","2024","2024-12-01 10:11:28","2024-12-14 18:49:22","","44462-44480","","","12","","IEEE Access","","","","","","","","English","","","Scopus","","","","Publisher: Institute of Electrical and Electronics Engineers Inc.","","","","Explainable AI; Fake news; Embeddings; Machine-learning; deep learning; Deep learning; Social networking (online); Learning algorithms; Long short-term memory; Classification (of information); Fake detection; Features extraction; Semantics; Interpretability; Decision making; Feature extraction; machine learning; Text processing; Transformer; Word embedding; Interpretability modeling; transformers; Brain; interpretability modeling; Statistics; Text-processing; word embeddings; manual_BERT; manual_LIME; manual_fake_news","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y3XUBGZN","journalArticle","2024","Zamil, Y.K.; Charkari, N.M.","Combating Fake News on Social Media: A Fusion Approach for Improved Detection and Interpretability","IEEE Access","","21693536 (ISSN)","10.1109/ACCESS.2023.3342843","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180284182&doi=10.1109%2fACCESS.2023.3342843&partnerID=40&md5=76c650e3c1377d15fa4b9bdef58f64f9","The proliferation of fake news on social media prompted research groups to develop statistical and learning methods to combat this menace. Deep learning techniques could not model and improve in terms of adopting multi-transformer topologies, enhancing interpretability, and coping with uncertainty. This article suggests a fusion strategy to create a more reliable fake news detection (FND) model by fusing text and image features. The different combinations of information in single and multi-modalities have been investigated to find optimal conditions. In this paper, we have employed pre-trained models of Electra and XLnet for text feature learning. Furthermore, ELA has been used to highlight the modified image features and EfficientNetB0 for image learning. To enhance the interpretability of the proposed model, the superpixels contributing to its interpretability are identified using the Local Interpretable Model-agnostic Explanations (LIME). Three well-known datasets (Weibo, MediaEval, and CASIA) have been used in this study. The results show that employing ELA and LIME in conjunction with the fusion of text and image features provides a solid and understandable solution to the FND issue in social media compared to other techniques.  © 2013 IEEE.","2024","2024-12-01 10:11:28","2024-12-14 18:57:27","","2074-2085","","","12","","IEEE Access","","","","","","","","English","","","Scopus","","","","Publisher: Institute of Electrical and Electronics Engineers Inc.","","","","Fake news; Fake news detection; Social media; Deep learning; Social networking (online); Lime; LIME; Features extraction; Feature extraction; Computational modelling; Transformer; fake news detection; Local interpretable model-agnostic explanation; efficientNetB0; Efficientnetb0; Error analysis; Error level analyse; error level analysis; Error levels; Social media news; Social medium news; Topology; Video; manual_LIME; manual_fake_news; manual_ELECTRA; manual_XLnet; manual_EfficientNetB0","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BM9NW7UK","journalArticle","2021","Li, Z.; Zhang, Q.; Du, X.; Ma, Y.; Wang, S.","Social media rumor refutation effectiveness: Evaluation, modelling and enhancement","Information Processing and Management","","03064573 (ISSN)","10.1016/j.ipm.2020.102420","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093975387&doi=10.1016%2fj.ipm.2020.102420&partnerID=40&md5=af620dcd80412ff56be34a978de81a1a","Motivated by the practical needs of enhancing social media rumor refutation effectiveness, this paper is dedicated to develop a proper rumor refutation effectiveness index (REI), identify key factors influencing REI and propose decision making suggestions for rumor refutation platforms. 298,118 pieces of comments and 185,209 pieces of the reposters’ verification status of 248 rumor refutation microblogs on Sina Weibo (the Chinese equivalent of Twitter) are collected during a 1-year period using a web crawler. To extract the text characteristics and analyze the sentiment of the rumor refutation microblogs, Natural Language Processing (NLP) approaches are applied. To explore the relationship between REI and the content and contextual factors of the rumor refutation microblogs, four regression models based on the collected data are established, namely linear regression model, Support Vector regression model (SVR), Extreme Gradient Boosting regression model (XGBoostRegressor) and Light Gradient Boosting Machine regression model (LGBMRegressor). The LGBMRegressor has the best goodness-of-fit among the compared regression models. Then, SHapley Additive exPlanations (SHAP) is employed to visualize and explain the LGBMRegressor results. Decision making suggestions for rumor refutation platforms on how to organize rumor refutation microblogs under different situations such as rumor category, author's influence and heat of topics are proposed. © 2020","2021","2024-12-01 10:11:33","2024-12-14 21:09:01","","","","1","58","","Inf. Process. Manage.","","","","","","","","English","","","Scopus","","","","Publisher: Elsevier Ltd","","","","Social networking (online); Natural Language Processing; Natural language processing systems; Decision making; Gradient boosting; NAtural language processing; Support vector regression; Web crawler; Big data of social media; Contextual factors; Goodness of fit; LGBMRegressor; Light gradients; Linear regression models; Regression model; Rumor refutation effectiveness; Support vector regression models; manual_SHAP; manual_rumours; manual_LGBMRegressor","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7VGKKA9H","journalArticle","2021","Szczepański, M.; Pawlicki, M.; Kozik, R.; Choraś, M.","New explainability method for BERT-based model in fake news detection","Scientific Reports","","20452322 (ISSN)","10.1038/s41598-021-03100-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120977480&doi=10.1038%2fs41598-021-03100-6&partnerID=40&md5=e5e546935962034e00c574df22300853","The ubiquity of social media and their deep integration in the contemporary society has granted new ways to interact, exchange information, form groups, or earn money—all on a scale never seen before. Those possibilities paired with the widespread popularity contribute to the level of impact that social media display. Unfortunately, the benefits brought by them come at a cost. Social Media can be employed by various entities to spread disinformation—so called ‘Fake News’, either to make a profit or influence the behaviour of the society. To reduce the impact and spread of Fake News, a diverse array of countermeasures were devised. These include linguistic-based approaches, which often utilise Natural Language Processing (NLP) and Deep Learning (DL). However, as the latest advancements in the Artificial Intelligence (AI) domain show, the model’s high performance is no longer enough. The explainability of the system’s decision is equally crucial in real-life scenarios. Therefore, the objective of this paper is to present a novel explainability approach in BERT-based fake news detectors. This approach does not require extensive changes to the system and can be attached as an extension for operating detectors. For this purposes, two Explainable Artificial Intelligence (xAI) techniques, Local Interpretable Model-Agnostic Explanations (LIME) and Anchors, will be used and evaluated on fake news data, i.e., short pieces of text forming tweets or headlines. This focus of this paper is on the explainability approach for fake news detectors, as the detectors themselves were part of previous works of the authors. © 2021, The Author(s).","2021","2024-12-01 10:11:32","2024-12-14 20:39:43","","","","1","11","","Sci. Rep.","","","","","","","","English","","","Scopus","","","","Publisher: Nature Research","","","","deep learning; social media; artificial intelligence; human; natural language processing; disinformation; article; Agnostic; profit; manual_BERT; manual_LIME; manual_fake_news; manual_anchor","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BTMEXN5N","conferencePaper","2021","Thiengburanathum, P.; Charoenkwan, P.","A Performance Comparison of Supervised Classifiers and Deep-learning Approaches for Predicting Toxicity in Thai Tweets","Jt. Int. Conf. Digit. Arts, Media Technol. ECTI North. Sect. Conf. Electr., Electron., Comput. Telecommun. Eng., ECTI DAMT NCON","978-166541569-9 (ISBN)","","10.1109/ECTIDAMTNCON51128.2021.9425718","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106616298&doi=10.1109%2fECTIDAMTNCON51128.2021.9425718&partnerID=40&md5=a412da3ae93d1df145aa43bafa7095f3","There are numerous tweeter user accounts in Thailand and many toxic comments are being generated every day on this platform. Sentimental Analysis can be used as a tool to identify toxic comments. In this study, two feature extraction techniques, including Bag of Words (BOW) and Term frequency-inverse document (TF-IDF), were investigated. Additionally, the performance of ten well-known traditional classifiers, along with three deep-learning approaches including Convolutional Neural Network (CNN), Long-short-Term memory (LSTM) and pretrained Bidirectional Encoder Representations (BERT), were compared with the public Toxicity Thai tweeter corpus the experiments reveal that by combining Bag of Words (BOW) with the Extra-Tree classifier, researchers were able to archive the highest F1-score of 0.72, classification accuracy rate of 72.27% and AUC value of 0.77 using the test set in contrast to other classifiers and other deep-learning techniques. Feature importance, correlation and impacts were also investigated through the use of SHapley Additive exPlanations (SHAP) diagram. © 2021 IEEE.","2021","2024-12-01 10:11:32","2024-12-14 18:47:13","","238-242","","","","","","","","","","","Institute of Electrical and Electronics Engineers Inc.","","English","","","Scopus","","","","Journal Abbreviation: Jt. Int. Conf. Digit. Arts, Media Technol. ECTI North. Sect. Conf. Electr., Electron., Comput. Telecommun. Eng., ECTI DAMT NCON","","","","Deep learning; Long short-term memory; Machine learning; Convolutional neural networks; Classification; Arts computing; Taxonomies; Learning approach; Learning techniques; Toxicity; Classification accuracy; Feature extraction techniques; Inverse documents; Performance comparison; Supervised classifiers; Term Frequency; Thai Sentimental Analysis; manual_BERT; manual_SHAP; manual_LSTM; manual_toxic","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2021 Joint 6th International Conference on Digital Arts, Media and Technology with 4th ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunication Engineering, ECTI DAMT and NCON 2021","","","","","","","","","","","","","","",""
"CLTE4LR3","conferencePaper","2021","Wich, M.; Mosca, E.; Gorniak, A.; Hingerl, J.; Groh, G.","Explainable Abusive Language Classification Leveraging User and Network Data","Lect. Notes Comput. Sci.","03029743 (ISSN); 978-303086516-0 (ISBN)","","10.1007/978-3-030-86517-7_30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115686015&doi=10.1007%2f978-3-030-86517-7_30&partnerID=40&md5=f746791f12d027647fe6b31fec142a40","Online hate speech is a phenomenon with considerable consequences for our society. Its automatic detection using machine learning is a promising approach to contain its spread. However, classifying abusive language with a model that purely relies on text data is limited in performance due to the complexity and diversity of speech (e.g., irony, sarcasm). Moreover, studies have shown that a significant amount of hate on social media platforms stems from online hate communities. Therefore, we develop an abusive language detection model leveraging user and network data to improve the classification performance. We integrate the explainable AI framework SHAP (SHapley Additive exPlanations) to alleviate the general issue of missing transparency associated with deep learning models, allowing us to assess the model’s vulnerability toward bias and systematic discrimination reliably. Furthermore, we evaluate our multimodel architecture on three datasets in two languages (i.e., English and German). Our results show that user-specific timeline and network data can improve the classification, while the additional explanations resulting from SHAP make the predictions of the model interpretable to humans. © 2021, Springer Nature Switzerland AG.","2021","2024-12-01 10:11:32","2024-12-14 19:32:10","","481-496","","","12979 LNAI","","","","","","","","Springer Science and Business Media Deutschland GmbH","","English","","","Scopus","","","","Journal Abbreviation: Lect. Notes Comput. Sci.","","","","Explainable AI; Deep learning; Social networking (online); Classification (of information); Automatic Detection; Shapley; Hate speech; Social network; Classification models; Abusive language; Classification model; Network data; User data; manual_BERT; manual_SHAP; manual_hate","","Dong Y.; Kourtellis N.; Hammer B.; Lozano J.A.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","","","","","","","","","","","","","","",""
"WBMF53NF","journalArticle","2022","Sun, R.; An, L.; Li, G.; Yu, C.","Predicting social media rumours in the context of public health emergencies","Journal of Information Science","","01655515 (ISSN)","10.1177/01655515221137879","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142854837&doi=10.1177%2f01655515221137879&partnerID=40&md5=a1523905782f7a382764e7b04e24cc7d","The spread of rumours on social media in the context of public health emergencies often distorts perceptions of public events and obstructs crisis management. Microblog entries about 28 rumour cases are collected on Sina Weibo during the COVID-19 outbreak. The Modality–Agency–Interactivity–Navigability model is used to identify the key factors of rumour prediction. To investigate the relationship among information modality, information content, information source and rumour identification, the binary logistic regression model is established based on the features of users and microblog entries. In addition, we propose a multi-feature rumour prediction model based on the Bidirectional Encoder Representations from Transformers (BERT) and Extreme Gradient Boosting (XGBoost) models. The proposed rumour prediction model has the best performance compared with other models. The feature importance is then calculated by the SHapley Additive exPlanations (SHAP), which can also explain the XGBoost results. It is shown that the likelihood that microblog entries are rumours decreases as the values of variables such as user influence and the positive sentiment of comments rise. Microblog entries posted on Thursdays or at noon are more probably to be rumours than those posted at other time. The proposed model can assist emergency management departments in establishing a feasible rumour prediction mechanism to guide public opinion against rumours. © The Author(s) 2022.","2022","2024-12-01 10:11:30","2024-12-14 20:47:48","","","","","","","J Inf Sci","","","","","","","","English","","","Scopus","","","","Publisher: SAGE Publications Ltd","","","","Social media; Social networking (online); Forecasting; Neural network models; social media; Xgboost; XGBoost; Neural network model; Health risks; Social aspects; Prediction modelling; Regression analysis; Public health; Crisis management; Health emergencies; Micro-blog; public health emergency; Public health emergency; Risk management; Rumor prediction; rumour prediction; Sina-weibo; manual_SHAP; manual_XGBoost; manual_rumours","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QIC7AHAQ","journalArticle","2022","Mehta, H.; Passi, K.","Social Media Hate Speech Detection Using Explainable Artificial Intelligence (XAI)","Algorithms","","19994893 (ISSN)","10.3390/a15080291","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137146369&doi=10.3390%2fa15080291&partnerID=40&md5=f703b2d38a651c891f9ed142d954769f","Explainable artificial intelligence (XAI) characteristics have flexible and multifaceted potential in hate speech detection by deep learning models. Interpreting and explaining decisions made by complex artificial intelligence (AI) models to understand the decision-making process of these model were the aims of this research. As a part of this research study, two datasets were taken to demonstrate hate speech detection using XAI. Data preprocessing was performed to clean data of any inconsistencies, clean the text of the tweets, tokenize and lemmatize the text, etc. Categorical variables were also simplified in order to generate a clean dataset for training purposes. Exploratory data analysis was performed on the datasets to uncover various patterns and insights. Various pre-existing models were applied to the Google Jigsaw dataset such as decision trees, k-nearest neighbors, multinomial naïve Bayes, random forest, logistic regression, and long short-term memory (LSTM), among which LSTM achieved an accuracy of 97.6%. Explainable methods such as LIME (local interpretable model—agnostic explanations) were applied to the HateXplain dataset. Variants of BERT (bidirectional encoder representations from transformers) model such as BERT + ANN (artificial neural network) with an accuracy of 93.55% and BERT + MLP (multilayer perceptron) with an accuracy of 93.67% were created to achieve a good performance in terms of explainability using the ERASER (evaluating rationales and simple English reasoning) benchmark. © 2022 by the authors.","2022","2024-12-01 10:11:30","2024-12-14 21:04:25","","","","8","15","","Algorithms","","","","","","","","English","","","Scopus","","","","Publisher: MDPI","","","","Social media; Learning models; Social networking (online); Long short-term memory; Explainable artificial intelligence; Lime; LIME; Multilayer neural networks; Speech recognition; BERT; Bidirectional encoder representation from transformer; Decision trees; Random forests; Benchmarking; Speech detection; Offensive languages; neural networks; Neural-networks; Hate speech detection; explainable artificial intelligence; hate speech detection; Intelligence models; Local interpretable model—agnostic explanation; Nearest neighbor search; offensive languages; manual_BERT; manual_LIME; manual_hate","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ACNZN5EE","conferencePaper","2022","Shakil, M.H.; Rabiul Alam, M.G.","Toxic Voice Classification Implementing CNN-LSTM & Employing Supervised Machine Learning Algorithms Through Explainable AI-SHAP","IEEE Int. Conf. Artif. Intell. Eng. Technol., IICAIET","978-166546837-4 (ISBN)","","10.1109/IICAIET55139.2022.9936775","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142222030&doi=10.1109%2fIICAIET55139.2022.9936775&partnerID=40&md5=9567061cc39629398f22268b45b284b1","Data innovation has advanced rapidly in recent years, and the network media has undergone several problematic changes. Places where consumers can express their thoughts through messages, photos, and notes, such as Facebook, Twitter, and Instagram, are gaining popularity. Unfortunately, it has become a place of toxic, insults, cyberbullying, and mysterious dangers. There is a lot of research here, but none has found a sufficient level of accuracy. This paper proposes a Convolutional Neural Network with Long Short-Term Memory (CNN-LSTM) and Natural Language Processing (NLP) fusion strategy that characterizes malicious and non-malicious remarks with a word embedding technique at an initial stage. And this model can categorize any voice data into six levels of classification. Furthermore, the processed dataset is applied to two traditional Machine Learning Algorithms (Random Forest and Extra Tress Algorithm) with an estimator (Logistic Regression) and interprets these algorithms with an Explainable AI (XAI)-SHAP. In the final step, two classifiers and the estimator are ensembled with Stacking Classifier, which is better than any previous activity.  © 2022 IEEE.","2022","2024-12-01 10:11:30","2024-12-14 21:20:42","","","","","","","","","","","","","Institute of Electrical and Electronics Engineers Inc.","","English","","","Scopus","","","","Journal Abbreviation: IEEE Int. Conf. Artif. Intell. Eng. Technol., IICAIET","","","","Explainable AI; Embeddings; Social networking (online); Long short-term memory; Supervised learning; Natural language processing systems; Natural languages; SHAP; Decision trees; Random forests; Convolutional neural network; Convolutional neural networks; Language processing; Natural language processing; NLP; Random Forest; Word embedding; Logistics regressions; Word Embedding; Tree algorithms; Logistic Regression; CNN-LSTM; Convolutional neural network with long short-term memory; Extra tree algorithm; Extra Trees Algorithm; Extra-trees; manual_SHAP; manual_LSTM; manual_toxic; manual_random_forest","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","4th IEEE International Conference on Artificial Intelligence in Engineering and Technology, IICAIET 2022","","","","","","","","","","","","","","",""
"RXB2R42E","conferencePaper","2022","Taleb, M.; Hamza, A.; Zouitni, M.; Burmani, N.; Lafkiar, S.; En-Nahnahi, N.","Detection of toxicity in social media based on Natural Language Processing methods","Int. Conf. Intell. Syst. Comput. Vis., ISCV","978-166549558-5 (ISBN)","","10.1109/ISCV54655.2022.9806096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134356178&doi=10.1109%2fISCV54655.2022.9806096&partnerID=40&md5=06a9aa0bd78e02c7aaaea7209b8abdfc","Comments on important websites, such as popular news portals or social media platforms, are among the main ways of virtual interaction. Unfortunately, the behavior of users on these websites often becomes rude or disrespectful, by spreading toxic comments which can muddle the proper functioning of these sites. The aim of this research is to detect these toxic comments, and to find parts, toxic spans, of these comments to which toxicity can be attributed. Thus, we explored and compared various classifiers belonging to three categories 'Machine Learning, Ensemble Learning and Deep Learning' and using different text representations. For detecting toxic spans in the comments, we applied an unsupervised method, we apply the Local Interpretable Model-Agnostic Explanations (LIME).The measures we used to evaluate our methods are accuracy, recall, and Fl-score. Our experiments showed that deep learning models performed unquestionably in the task of detecting toxic comments. The LSTM models with the Globe representation and LSTM with FastText were able to produce a higher F1 and accuracy compared to the other models used. For Toxic spans detction, the higher scores were obtained when combining LIME with classifier LSTM(GloVe) with an accuracy of 98% to identify the toxic spans.  © 2022 IEEE.","2022","2024-12-01 10:11:30","2024-12-14 19:16:10","","","","","","","","","","","","","Institute of Electrical and Electronics Engineers Inc.","","English","","","Scopus","","","","Journal Abbreviation: Int. Conf. Intell. Syst. Comput. Vis., ISCV","","","","Social media; Learning systems; Deep learning; Social networking (online); Deep Learning; Learning algorithms; Long short-term memory; Lime; LIME; Natural language processing systems; Natural languages; Language processing; LSTM; Local interpretable model-agnostic explanation; Toxicity; fastText; Fasttext; Globe; Processing method; Toxic span; Toxic spans; manual_LSTM; manual_LIME; manual_toxic","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2022 International Conference on Intelligent Systems and Computer Vision, ISCV 2022","","","","","","","","","","","","","","",""
"NGH6QXGB","journalArticle","2023","Catelli, R.; Bevilacqua, L.; Mariniello, N.; Carlo, V.S.D.; Magaldi, M.; Fujita, H.; De Pietro, G.; Esposito, M.","A New Italian Cultural Heritage Data Set: Detecting Fake Reviews With BERT and ELECTRA Leveraging the Sentiment","IEEE Access","","21693536 (ISSN)","10.1109/ACCESS.2023.3277490","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160224925&doi=10.1109%2fACCESS.2023.3277490&partnerID=40&md5=cbe82fa9cb6e38792f538f14c7f82b0f","The growth of the online review phenomenon, which has expanded from specialised trade magazines to end users via online platforms, has also increasingly involved the cultural heritage of countries, a source of tourism and growth driver of local economies. Unfortunately, this has been paralleled by the emergence and spread of the phenomenon of fake reviews, against which the scientific world has developed language models capable of distinguishing them from the truthful. The application of such models, often based on deep neural networks with transformer-type architectures, is however limited by the availability of local language data sets for specific domains, useful for both training and verification. The purpose of this article is twofold. Firstly, a new data set was created in the Italian language, generally considered low-resource, relating to the domain of cultural heritage in Italy, by collecting reviews available online, reorganising them in the form of a data set usable by the language models. Secondly, a baseline of results for the detection of misleading reviews was constructed by exploiting two widely used language models, namely BERT and ELECTRA. The performance achieved is interesting, around 95% accuracy and F1 score, using data set splits between training and testing of 80/20 and 90/10. In addition, SHAP was used as a tool to support the explicability of AI models: in this way, it was possible to show the usefulness of sentiment analysis as a support for the recognition of deceptiveness.  © 2013 IEEE.","2023","2024-12-01 10:11:30","2024-12-14 18:47:27","","52214-52225","","","11","","IEEE Access","","","","","","","","English","","","Scopus","","","","Publisher: Institute of Electrical and Electronics Engineers Inc.","","","","Fake news; Data mining; Deep learning; Sentiment analysis; Deep neural networks; Computational linguistics; Support vector machines; Statistical tests; data set; Modeling languages; Bit error rate; Bit-error rate; Support vectors machine; Fake review; sentiment analysis; Data set; Cultural aspects; Biological system modeling; Biological systems; Cultural heritages; deceptive; Deceptive; fake reviews; Italian cultural heritage; manual_BERT; manual_SHAP; manual_fake_reviews","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BDEC79TX","journalArticle","2023","Han, S.; Wang, H.; Li, W.; Zhang, H.; Zhuang, L.","Explainable knowledge integrated sequence model for detecting fake online reviews","Applied Intelligence","","0924669X (ISSN)","10.1007/s10489-022-03822-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134075412&doi=10.1007%2fs10489-022-03822-8&partnerID=40&md5=a64e00d8b74f5b53d84f5a34b369a360","Online reviews have a great influence on customers’ shopping decisions. However, countless fake reviews are posted on shopping platforms, which seriously interfere with customers’ shopping decisions and pollute the fair e-commerce environment. In this paper, we propose EKI-SM, an explainable knowledge integrated sequence model, to detect fake reviews. Compared with existing models, the EKI-SM displays four advantages: 1) It integrates a set of important knowledge and learns high-dimensional word embedding from reviews to guide fake review detection tasks; in addition, this knowledge explains the results of the model. 2) It learns a continuous sequence model from discrete observations with high-dimensional features, which helps to learn more discriminating fake review features. 3) It fuses the one-dimensional convolutional network, the long short-term memory network, and the residual connector to capture the local and global dependency of the sequence and make the prediction model more robust. 4) Inspired by the idea of interpretable deep learning, we explain the EKI-SM and find the important critical words for detecting fake online reviews, which derive some interesting insights. Experiments on actual fake review datasets demonstrate that the EKI-SM achieves higher accuracy in fake review detection than that of other state-of-the-art methods; indeed, it benefits from the integration of knowledge and multi-modal features. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","2023","2024-12-01 10:11:30","2024-12-15 09:19:52","","6953-6965","","6","53","","Appl Intell","","","","","","","","English","","","Scopus","","","","Publisher: Springer","","","","Deep learning; E- commerces; Embeddings; Explainable sequence model; Fake detection; Fake review detection; Fake review detections; Knowledge integrated; Knowledge integration; Learn+; manual_BERT; manual_EKI-SM; manual_fake_reviews; manual_LIME; Online reviews; Sequence models; Word embedding","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V69JM9EC","conferencePaper","2023","Gongane, V.U.; Munot, M.V.; Anuse, A.","Explainable AI for Reliable Detection of Cyberbullying","IEEE Pune Sect. Int. Conf., PuneCon","979-835032420-4 (ISBN)","","10.1109/PuneCon58714.2023.10450132","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187799981&doi=10.1109%2fPuneCon58714.2023.10450132&partnerID=40&md5=d8395bbe15c8f1f889ca3de599a0ed6a","The advent of Internet has brought a pivotal revolution in communication technology. This revolution is observed through tremendous use of devices like cell phones and social networking websites that provide a platform for people to interact virtually. The use of social networking websites has provided economical, educational and societal benefits. The ease of access to information and the liberty to openly share and publish information on social networking sites also has a downside. Past decade has witnessed an upsurge of objectionable and cyberbullying content shared on social media. The massiveness and the sensitive bullying content shared on social media platforms make manual method of detecting such content practically challenging. Automated approaches like Artificial Intelligence (AI) based techniques are widely being adopted to detect and moderate online cyberbullying content. Natural Language Processing (NLP) and Deep Learning (DL) are at forefront in automating the detection of cyberbullying content. Inspite of the wide adoption of DL models for detection of bullying content, the decision and predictions made by these models are difficult to comprehend. Explainable AI (XAI) is a promising field that provide interpretations to the decision made by DL models. Local Interpretable Model-Agnostic Explanations (LIME) XAI technique provide better explanations by highlighting the most pertinent features that contributed to model's decision. This paper proposes a unified BiLSTM-LIME model for multiclass classification of cyberbullying content on Twitter platform.  © 2023 IEEE.","2023","2024-12-01 10:11:29","2024-12-14 19:33:26","","","","","","","","","","","","","Institute of Electrical and Electronics Engineers Inc.","","English","","","Scopus","","","","Journal Abbreviation: IEEE Pune Sect. Int. Conf., PuneCon","","","","Deep learning; Learning models; Social networking (online); Lime; LIME; Natural language processing systems; BERT; Computer crime; CNN; BiLSTM; Social-networking; Cyber bullying; Cyberbullying; Local interpretable model-agnostic explanation; Cell phone; Communicationtechnology; Economical benefits; Reliable detection; manual_BERT; manual_LIME; manual_cyberbullying","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2023 IEEE Pune Section International Conference, PuneCon 2023","","","","","","","","","","","","","","",""
"GSRJQLGH","conferencePaper","2023","Qureshi, M.D.M.; Qureshi, M.A.; Rashwan, W.","Toward Inclusive Online Environments: Counterfactual-Inspired XAI for Detecting and Interpreting Hateful and Offensive Tweets","Commun. Comput. Info. Sci.","18650929 (ISSN); 978-303144069-4 (ISBN)","","10.1007/978-3-031-44070-0_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176011419&doi=10.1007%2f978-3-031-44070-0_5&partnerID=40&md5=3078f123e7c851ccc8cdd56b0db1f00f","The prevalence of hate speech and offensive language on social media platforms such as Twitter has significant consequences, ranging from psychological harm to the polarization of societies. Consequently, social media companies have implemented content moderation measures to curb harmful or discriminatory language. However, a lack of consistency and transparency hinders their ability to achieve desired outcomes. This article evaluates various ML models, including an ensemble, Explainable Boosting Machine (EBM), and Linear Support Vector Classifier (SVC), on a public dataset of 24,792 tweets by T. Davidson, categorizing tweets into three classes: hate, offensive, and neither. The top-performing model achieves a weighted F1-Score of 0.90. Furthermore, this article interprets the output of the best-performing model using LIME and SHAP, elucidating how specific words and phrases within a tweet contextually impact its classification. This analysis helps to shed light on the linguistic aspects of hate and offense. Additionally, we employ LIME to present a suggestive counterfactual approach, proposing no-hate alternatives for a tweet to further explain the influence of word choices in context. Limitations of the study include the potential for biased results due to dataset imbalance, which future research may address by exploring more balanced datasets or leveraging additional features. Ultimately, through these explanations, this work aims to promote digital literacy and foster an inclusive online environment that encourages informed and responsible use of digital technologies (A GitHub repository containing code, data, and pre-trained models is available at: https://github.com/DeedahwarMazhar/XAI-Counterfactual-Hate-Speech ). © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","2023","2024-12-01 10:11:29","2024-12-14 21:15:38","","97-119","","","1903 CCIS","","","","","","","","Springer Science and Business Media Deutschland GmbH","","English","","","Scopus","","","","Journal Abbreviation: Commun. Comput. Info. Sci.","","","","XAI; Social media; Machine-learning; Social media platforms; Social networking (online); Machine learning; Classification (of information); Lime; LIME; SHAP; Machine Learning; E-learning; Offensive languages; Counterfactuals; Counterfactual; Digital literacies; Digital Literacy; Media companies; Online environments; manual_SHAP; manual_LIME; manual_hate; manual_counterfactual; manual_Linear_SVC","","Longo L.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Communications in Computer and Information Science","","","","","","","","","","","","","","",""